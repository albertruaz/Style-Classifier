#!/usr/bin/env python3
# test_trained_model.py

import os
import json
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    precision_recall_curve, roc_curve, mean_squared_error, 
    mean_absolute_error, r2_score
)
from torch.utils.data import DataLoader
from tqdm import tqdm
import argparse
from typing import Dict, List, Tuple, Any
from datetime import datetime

# ë¡œì»¬ ëª¨ë“ˆ
from database import DatabaseManager
from dataset.morigirl_dataset import MorigirlDataset
from dataset.product_score_dataset import ProductScoreDataset
from model.morigirl_model import MorigirlModel
from model.score_prediction_model import ScorePredictionModel
from utils.train_utils import load_checkpoint

class ModelTester:
    """í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: str = "./config.json"):
        with open(config_path, 'r') as f:
            self.config = json.load(f)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"âœ… í…ŒìŠ¤íŠ¸ í™˜ê²½: {self.device}")
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.results_dir = f"./results/test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(self.results_dir, exist_ok=True)
        
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {self.results_dir}")

    def load_model(self, checkpoint_path: str, task_type: str) -> nn.Module:
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”©: {checkpoint_path}")
        
        checkpoint = load_checkpoint(checkpoint_path)
        
        # ëª¨ë¸ ìƒì„±
        if task_type == "morigirl":
            model = MorigirlModel(
                input_dim=1024,
                hidden_dim=self.config.get('model', {}).get('hidden_dim', 512),
                num_classes=2,
                dropout_rate=self.config.get('model', {}).get('dropout_rate', 0.3)
            )
        elif task_type == "score":
            model = ScorePredictionModel(
                image_dim=1024,
                hidden_dim=self.config.get('model', {}).get('hidden_dim', 512),
                dropout_rate=self.config.get('model', {}).get('dropout_rate', 0.3)
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒœìŠ¤í¬ íƒ€ì…: {task_type}")
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Epoch: {checkpoint.get('epoch', 'N/A')})")
        return model

    def setup_dataset(self, task_type: str, mode: str = "test") -> DataLoader:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„¤ì •"""
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„¤ì • ì¤‘... (Task: {task_type})")
        
        if task_type == "morigirl":
            dataset = MorigirlDataset(
                config=self.config,
                mode=mode
            )
        elif task_type == "score":
            dataset = ProductScoreDataset(
                config=self.config,
                mode=mode
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒœìŠ¤í¬ íƒ€ì…: {task_type}")
        
        # ì „ì²´ ë°ì´í„°ì…‹ì„ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í•  í•„ìš”)
        test_loader = DataLoader(
            dataset,
            batch_size=self.config.get('training', {}).get('batch_size', 32),
            shuffle=False,
            num_workers=4,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(dataset):,}ê°œ")
        return test_loader

    def predict_batch(self, model: nn.Module, data_loader: DataLoader, 
                     task_type: str) -> Tuple[np.ndarray, np.ndarray]:
        """ë°°ì¹˜ ì˜ˆì¸¡"""
        predictions = []
        targets = []
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc="ì˜ˆì¸¡ ì¤‘"):
                if task_type == "morigirl":
                    image_vectors, labels = batch
                    image_vectors = image_vectors.to(self.device)
                    
                    outputs = model(image_vectors)
                    probs = torch.softmax(outputs, dim=1)
                    
                    predictions.extend(probs.cpu().numpy())
                    targets.extend(labels.numpy())
                    
                elif task_type == "score":
                    image_vectors, scores = batch
                    image_vectors = image_vectors.to(self.device)
                    
                    outputs = model(image_vectors)
                    
                    predictions.extend(outputs.cpu().numpy().flatten())
                    targets.extend(scores.numpy().flatten())
        
        return np.array(predictions), np.array(targets)

    def evaluate_classification(self, predictions: np.ndarray, targets: np.ndarray) -> Dict[str, Any]:
        """ë¶„ë¥˜ ëª¨ë¸ í‰ê°€"""
        print(f"ğŸ¯ ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        
        # ì˜ˆì¸¡ í´ë˜ìŠ¤ (í™•ë¥  -> í´ë˜ìŠ¤)
        pred_classes = np.argmax(predictions, axis=1)
        pred_probs = predictions[:, 1]  # ëª¨ë¦¬ê±¸ í™•ë¥ 
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        accuracy = np.mean(pred_classes == targets)
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        report = classification_report(targets, pred_classes, output_dict=True)
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(targets, pred_classes)
        
        # ROC AUC
        try:
            auc = roc_auc_score(targets, pred_probs)
        except:
            auc = None
        
        # ROC ê³¡ì„ 
        fpr, tpr, _ = roc_curve(targets, pred_probs)
        
        # Precision-Recall ê³¡ì„ 
        precision, recall, _ = precision_recall_curve(targets, pred_probs)
        
        results = {
            'accuracy': accuracy,
            'classification_report': report,
            'confusion_matrix': cm,
            'auc': auc,
            'roc_curve': (fpr, tpr),
            'precision_recall_curve': (precision, recall)
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"  ğŸ“Š ì •í™•ë„: {accuracy:.4f}")
        if auc:
            print(f"  ğŸ“Š AUC: {auc:.4f}")
        print(f"  ğŸ“Š ì •ë°€ë„ (Class 1): {report['1']['precision']:.4f}")
        print(f"  ğŸ“Š ì¬í˜„ìœ¨ (Class 1): {report['1']['recall']:.4f}")
        print(f"  ğŸ“Š F1-Score (Class 1): {report['1']['f1-score']:.4f}")
        
        return results

    def evaluate_regression(self, predictions: np.ndarray, targets: np.ndarray) -> Dict[str, Any]:
        """íšŒê·€ ëª¨ë¸ í‰ê°€"""
        print(f"ğŸ¯ íšŒê·€ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        mse = mean_squared_error(targets, predictions)
        mae = mean_absolute_error(targets, predictions)
        rmse = np.sqrt(mse)
        r2 = r2_score(targets, predictions)
        
        # ì˜¤ì°¨ ë¶„ì„
        errors = predictions - targets
        error_std = np.std(errors)
        error_mean = np.mean(errors)
        
        results = {
            'mse': mse,
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'error_mean': error_mean,
            'error_std': error_std,
            'predictions': predictions,
            'targets': targets,
            'errors': errors
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"  ğŸ“Š MSE: {mse:.4f}")
        print(f"  ğŸ“Š MAE: {mae:.4f}")
        print(f"  ğŸ“Š RMSE: {rmse:.4f}")
        print(f"  ğŸ“Š RÂ²: {r2:.4f}")
        print(f"  ğŸ“Š ì˜¤ì°¨ í‰ê· : {error_mean:.4f}")
        print(f"  ğŸ“Š ì˜¤ì°¨ í‘œì¤€í¸ì°¨: {error_std:.4f}")
        
        return results

    def visualize_classification_results(self, results: Dict[str, Any], save_prefix: str):
        """ë¶„ë¥˜ ê²°ê³¼ ì‹œê°í™”"""
        print("ğŸ“ˆ ë¶„ë¥˜ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
        
        # 1. í˜¼ë™ í–‰ë ¬
        plt.figure(figsize=(8, 6))
        sns.heatmap(results['confusion_matrix'], annot=True, fmt='d', cmap='Blues')
        plt.title('Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, f'{save_prefix}_confusion_matrix.png'), dpi=300)
        plt.close()
        
        # 2. ROC ê³¡ì„ 
        if results['auc']:
            plt.figure(figsize=(8, 6))
            fpr, tpr = results['roc_curve']
            plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {results["auc"]:.3f})')
            plt.plot([0, 1], [0, 1], 'k--', label='Random')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title('ROC Curve')
            plt.legend()
            plt.grid(True)
            plt.tight_layout()
            plt.savefig(os.path.join(self.results_dir, f'{save_prefix}_roc_curve.png'), dpi=300)
            plt.close()
        
        # 3. Precision-Recall ê³¡ì„ 
        plt.figure(figsize=(8, 6))
        precision, recall = results['precision_recall_curve']
        plt.plot(recall, precision)
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve')
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, f'{save_prefix}_pr_curve.png'), dpi=300)
        plt.close()

    def visualize_regression_results(self, results: Dict[str, Any], save_prefix: str):
        """íšŒê·€ ê²°ê³¼ ì‹œê°í™”"""
        print("ğŸ“ˆ íšŒê·€ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
        
        predictions = results['predictions']
        targets = results['targets']
        errors = results['errors']
        
        # 1. ì˜ˆì¸¡ vs ì‹¤ì œê°’ ì‚°ì ë„
        plt.figure(figsize=(10, 4))
        
        plt.subplot(1, 2, 1)
        plt.scatter(targets, predictions, alpha=0.6)
        plt.plot([targets.min(), targets.max()], [targets.min(), targets.max()], 'r--')
        plt.xlabel('Actual Values')
        plt.ylabel('Predicted Values')
        plt.title(f'Predictions vs Actual (RÂ² = {results["r2"]:.3f})')
        plt.grid(True)
        
        # 2. ì˜¤ì°¨ íˆìŠ¤í† ê·¸ë¨
        plt.subplot(1, 2, 2)
        plt.hist(errors, bins=50, alpha=0.7, edgecolor='black')
        plt.xlabel('Prediction Error')
        plt.ylabel('Frequency')
        plt.title(f'Error Distribution (Î¼ = {results["error_mean"]:.3f})')
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, f'{save_prefix}_regression_analysis.png'), dpi=300)
        plt.close()
        
        # 3. ì”ì°¨ í”Œë¡¯
        plt.figure(figsize=(8, 6))
        plt.scatter(predictions, errors, alpha=0.6)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('Predicted Values')
        plt.ylabel('Residuals')
        plt.title('Residual Plot')
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, f'{save_prefix}_residual_plot.png'), dpi=300)
        plt.close()

    def save_detailed_results(self, results: Dict[str, Any], task_type: str, save_prefix: str):
        """ìƒì„¸ ê²°ê³¼ ì €ì¥"""
        print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # JSONìœ¼ë¡œ ë©”íŠ¸ë¦­ ì €ì¥
        metrics = {}
        
        if task_type == "morigirl":
            metrics = {
                'accuracy': float(results['accuracy']),
                'auc': float(results['auc']) if results['auc'] else None,
                'classification_report': results['classification_report']
            }
        elif task_type == "score":
            metrics = {
                'mse': float(results['mse']),
                'mae': float(results['mae']),
                'rmse': float(results['rmse']),
                'r2': float(results['r2']),
                'error_mean': float(results['error_mean']),
                'error_std': float(results['error_std'])
            }
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        with open(os.path.join(self.results_dir, f'{save_prefix}_metrics.json'), 'w') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        if task_type == "score":
            results_df = pd.DataFrame({
                'actual': results['targets'],
                'predicted': results['predictions'],
                'error': results['errors']
            })
            results_df.to_csv(os.path.join(self.results_dir, f'{save_prefix}_predictions.csv'), index=False)

    def run_comprehensive_test(self, checkpoint_path: str, task_type: str):
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"ğŸš€ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘ (Task: {task_type})")
        
        # ëª¨ë¸ ë¡œë“œ
        model = self.load_model(checkpoint_path, task_type)
        
        # ë°ì´í„°ì…‹ ì„¤ì •
        test_loader = self.setup_dataset(task_type, mode="test")
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions, targets = self.predict_batch(model, test_loader, task_type)
        
        # ì„±ëŠ¥ í‰ê°€
        if task_type == "morigirl":
            results = self.evaluate_classification(predictions, targets)
            self.visualize_classification_results(results, "classification")
        elif task_type == "score":
            results = self.evaluate_regression(predictions, targets)
            self.visualize_regression_results(results, "regression")
        
        # ê²°ê³¼ ì €ì¥
        self.save_detailed_results(results, task_type, task_type)
        
        print(f"ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.results_dir}")
        
        return results

    def run_single_inference(self, checkpoint_path: str, task_type: str, 
                           product_ids: List[int] = None):
        """ê°œë³„ ìƒí’ˆ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ” ê°œë³„ ìƒí’ˆ ì¶”ë¡  í…ŒìŠ¤íŠ¸ (Task: {task_type})")
        
        # ëª¨ë¸ ë¡œë“œ
        model = self.load_model(checkpoint_path, task_type)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìƒí’ˆ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        db_manager = DatabaseManager()
        
        try:
            if not product_ids:
                # ëœë¤í•˜ê²Œ 10ê°œ ìƒí’ˆ ì„ íƒ
                session = db_manager.mysql.Session()
                from sqlalchemy import text
                
                sql = text("""
                    SELECT p.id, p.name, p.main_image
                    FROM vingle.product p
                    JOIN product_vectors pv ON p.id = pv.id
                    WHERE p.status = 'SALE'
                    ORDER BY RAND()
                    LIMIT 10
                """)
                
                result = session.execute(sql)
                product_ids = [row[0] for row in result.fetchall()]
                session.close()
            
            print(f"ğŸ“¦ í…ŒìŠ¤íŠ¸ ìƒí’ˆ: {len(product_ids)}ê°œ")
            
            # ë²¡í„° ì¡°íšŒ
            vectors_data = db_manager.vector_db.query_product_vectors(product_ids)
            
            results = []
            
            for product_id in product_ids:
                if product_id not in vectors_data:
                    print(f"âš ï¸ ìƒí’ˆ {product_id}: ë²¡í„° ì—†ìŒ")
                    continue
                
                # ë²¡í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
                vector = torch.FloatTensor(vectors_data[product_id]).unsqueeze(0).to(self.device)
                
                # ì¶”ë¡ 
                with torch.no_grad():
                    output = model(vector)
                    
                    if task_type == "morigirl":
                        probs = torch.softmax(output, dim=1)
                        morigirl_prob = probs[0][1].item()
                        prediction = "ëª¨ë¦¬ê±¸" if morigirl_prob > 0.5 else "ì¼ë°˜"
                        confidence = morigirl_prob if morigirl_prob > 0.5 else 1 - morigirl_prob
                        
                        result = {
                            'product_id': product_id,
                            'prediction': prediction,
                            'morigirl_probability': morigirl_prob,
                            'confidence': confidence
                        }
                        
                    elif task_type == "score":
                        score = output[0].item()
                        result = {
                            'product_id': product_id,
                            'predicted_score': score
                        }
                    
                    results.append(result)
                    print(f"  ìƒí’ˆ {product_id}: {result}")
            
            # ê²°ê³¼ ì €ì¥
            results_df = pd.DataFrame(results)
            results_df.to_csv(os.path.join(self.results_dir, 'single_inference_results.csv'), index=False)
            
            return results
            
        finally:
            db_manager.dispose_all()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--checkpoint', type=str, required=True, help='ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
    parser.add_argument('--task', choices=['morigirl', 'score'], required=True, 
                       help='í…ŒìŠ¤íŠ¸í•  íƒœìŠ¤í¬')
    parser.add_argument('--config', type=str, default='./config.json', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--mode', choices=['comprehensive', 'single'], default='comprehensive',
                       help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (comprehensive: ì „ì²´ í‰ê°€, single: ê°œë³„ ìƒí’ˆ)')
    parser.add_argument('--product-ids', nargs='+', type=int, help='í…ŒìŠ¤íŠ¸í•  ìƒí’ˆ IDë“¤ (single ëª¨ë“œ)')
    
    args = parser.parse_args()
    
    try:
        tester = ModelTester(args.config)
        
        if args.mode == 'comprehensive':
            results = tester.run_comprehensive_test(args.checkpoint, args.task)
            print(f"\nâœ… ì¢…í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            
        elif args.mode == 'single':
            results = tester.run_single_inference(args.checkpoint, args.task, args.product_ids)
            print(f"\nâœ… ê°œë³„ ì¶”ë¡  ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    main() 