#!/usr/bin/env python3
# train_model.py

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from datetime import datetime
from tqdm import tqdm
from typing import Dict, Tuple
import argparse
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, classification_report

# ë¡œì»¬ ëª¨ë“ˆ
from prepare_training_data import MorigirlDataProcessor, MorigirlDataset
from model.morigirl_model import MoriGirlVectorClassifier, get_model_info

class MoriGirlTrainer:
    """ëª¨ë¦¬ê±¸ ë²¡í„° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 data_path: str = "data/morigirl_50",
                 experiment_name: str = None):
        
        self.data_path = data_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ì‹¤í—˜ ì´ë¦„ ì„¤ì • (ì›”ì¼ì‹œë¶„_ëœë¤2ìë¦¬)
        if experiment_name is None:
            import random
            date_str = datetime.now().strftime('%m%d%H%M')  # ì›”ì¼ì‹œë¶„
            random_num = random.randint(10, 99)  # ëœë¤ 2ìë¦¬
            self.experiment_name = f"{date_str}_{random_num:02d}"
        else:
            self.experiment_name = experiment_name
        
        # result í´ë”ì— ì‹¤í—˜ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        self.result_dir = f"result/{self.experiment_name}"
        self.checkpoint_dir = f"{self.result_dir}/checkpoints"
        
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        print(f"ğŸš€ ëª¨ë¦¬ê±¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print(f"  - ì‹¤í—˜ëª…: {self.experiment_name}")
        print(f"  - ë°ì´í„° ê²½ë¡œ: {self.data_path}")
        print(f"  - ê²°ê³¼ í´ë”: {self.result_dir}")
        print(f"  - ë””ë°”ì´ìŠ¤: {self.device}")

    def setup_datasets(self, 
                      test_size: float = 0.2,
                      val_size: float = 0.1,
                      batch_size: int = 32) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """ë°ì´í„°ì…‹ ì„¤ì •"""
        print(f"\nğŸ“Š ë°ì´í„°ì…‹ ì„¤ì •")
        
        # ë°ì´í„° ì²˜ë¦¬ê¸° ìƒì„± ë° ë¡œë”©
        processor = MorigirlDataProcessor(self.data_path)
        if not processor.load_npy_files():
            raise RuntimeError("ë°ì´í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # Train/Test ë¶„í• 
        train_dataset, test_dataset = processor.create_train_test_split(
            test_size=test_size, random_state=42
        )
        
        # Trainì—ì„œ Validation ë¶„í• 
        val_size_adjusted = val_size / (1 - test_size)  # ì „ì²´ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì¡°ì •
        train_vectors = train_dataset.vectors.numpy()
        train_labels = train_dataset.labels.numpy()
        train_ids = train_dataset.product_ids
        
        from sklearn.model_selection import train_test_split
        X_train, X_val, y_train, y_val, ids_train, ids_val = train_test_split(
            train_vectors, train_labels, train_ids,
            test_size=val_size_adjusted,
            random_state=42,
            stratify=train_labels
        )
        
        # ìƒˆë¡œìš´ Dataset ê°ì²´ ìƒì„±
        train_dataset = MorigirlDataset(X_train, y_train, ids_train)
        val_dataset = MorigirlDataset(X_val, y_val, ids_val)
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• í•´ê²°)
        pos_count = torch.sum(train_dataset.labels).item()
        neg_count = len(train_dataset) - pos_count
        self.pos_weight = torch.tensor(neg_count / pos_count).to(self.device)
        print(f"ğŸ“Š í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {self.pos_weight.item():.2f}")
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_loader, _ = processor.create_dataloaders(train_dataset, val_dataset, batch_size)
        _, val_loader = processor.create_dataloaders(val_dataset, test_dataset, batch_size)
        _, test_loader = processor.create_dataloaders(test_dataset, test_dataset, batch_size)
        
        return train_loader, val_loader, test_loader

    def setup_model(self, **model_kwargs) -> nn.Module:
        """ëª¨ë¸ ì„¤ì •"""
        print(f"\nğŸ§  ëª¨ë¸ ì„¤ì •")
        
        model = MoriGirlVectorClassifier(**model_kwargs)
        model.to(self.device)
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        get_model_info(model)
        
        return model

    def train_epoch(self, model: nn.Module, train_loader: DataLoader, 
                   criterion: nn.Module, optimizer: optim.Optimizer) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        model.train()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        all_probs = []
        
        progress_bar = tqdm(train_loader, desc="í•™ìŠµ")
        
        for batch_idx, batch in enumerate(progress_bar):
            vectors = batch['vector'].to(self.device)
            labels = batch['label'].to(self.device).unsqueeze(1)  # (batch_size, 1)
            
            # Forward pass
            outputs = model(vectors)  # ì´ë¯¸ sigmoid ì ìš©ë¨
            loss = criterion(outputs, labels)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            total_loss += loss.item()
            probs = outputs.detach().cpu().numpy()
            preds = (probs > 0.5).astype(int)
            
            all_probs.extend(probs.flatten())
            all_preds.extend(preds.flatten())
            all_labels.extend(labels.cpu().numpy().flatten())
            
            # Progress bar ì—…ë°ì´íŠ¸
            avg_loss = total_loss / (batch_idx + 1)
            progress_bar.set_postfix({'Loss': f'{avg_loss:.4f}'})
        
        # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = accuracy_score(all_labels, all_preds)
        try:
            auc = roc_auc_score(all_labels, all_probs)
        except:
            auc = 0.0
        
        return {
            'loss': total_loss / len(train_loader),
            'accuracy': accuracy,
            'auc': auc
        }

    def validate_epoch(self, model: nn.Module, val_loader: DataLoader, 
                      criterion: nn.Module) -> Dict[str, float]:
        """í•œ ì—í¬í¬ ê²€ì¦"""
        model.eval()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        all_probs = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="ê²€ì¦"):
                vectors = batch['vector'].to(self.device)
                labels = batch['label'].to(self.device).unsqueeze(1)
                
                outputs = model(vectors)
                loss = criterion(outputs, labels)
                
                total_loss += loss.item()
                probs = outputs.cpu().numpy()
                preds = (probs > 0.5).astype(int)
                
                all_probs.extend(probs.flatten())
                all_preds.extend(preds.flatten())
                all_labels.extend(labels.cpu().numpy().flatten())
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = accuracy_score(all_labels, all_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels, all_preds, average='binary', zero_division=0
        )
        try:
            auc = roc_auc_score(all_labels, all_probs)
        except:
            auc = 0.0
        
        return {
            'loss': total_loss / len(val_loader),
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc
        }

    def save_checkpoint(self, model: nn.Module, optimizer: optim.Optimizer, 
                       epoch: int, metrics: Dict[str, float], is_best: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'metrics': metrics,
            'experiment_name': self.experiment_name
        }
        
        # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')
        torch.save(checkpoint, checkpoint_path)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if is_best:
            best_path = os.path.join(self.checkpoint_dir, 'best_model.pth')
            torch.save(checkpoint, best_path)
            print(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_path}")

    def train(self, 
              epochs: int = 50,
              learning_rate: float = 0.001,
              weight_decay: float = 0.01,
              patience: int = 10,
              **model_kwargs):
        """ëª¨ë¸ í•™ìŠµ"""
        
        # ë°ì´í„°ì…‹ ì„¤ì •
        train_loader, val_loader, test_loader = self.setup_datasets()
        
        # ëª¨ë¸ ì„¤ì •
        model = self.setup_model(**model_kwargs)
        
        # ì†ì‹¤í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        criterion = nn.BCELoss(weight=self.pos_weight if self.pos_weight.item() > 1 else None)
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5)
        
        # í•™ìŠµ ê¸°ë¡
        best_val_acc = 0.0
        patience_counter = 0
        train_history = []
        
        print(f"\nğŸ¯ í•™ìŠµ ì‹œì‘ (ì´ {epochs} ì—í¬í¬)")
        
        for epoch in range(epochs):
            print(f"\n=== Epoch {epoch+1}/{epochs} ===")
            
            # í•™ìŠµ
            train_metrics = self.train_epoch(model, train_loader, criterion, optimizer)
            
            # ê²€ì¦
            val_metrics = self.validate_epoch(model, val_loader, criterion)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            scheduler.step(val_metrics['accuracy'])
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"í•™ìŠµ - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, AUC: {train_metrics['auc']:.4f}")
            print(f"ê²€ì¦ - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}, AUC: {val_metrics['auc']:.4f}")
            
            # ìµœê³  ì„±ëŠ¥ í™•ì¸
            is_best = val_metrics['accuracy'] > best_val_acc
            if is_best:
                best_val_acc = val_metrics['accuracy']
                patience_counter = 0
            else:
                patience_counter += 1
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            self.save_checkpoint(model, optimizer, epoch, val_metrics, is_best)
            
            # ê¸°ë¡ ì €ì¥
            train_history.append({
                'epoch': epoch + 1,
                'train': train_metrics,
                'val': val_metrics
            })
            
            # Early stopping
            if patience_counter >= patience:
                print(f"â° Early stopping at epoch {epoch+1} (patience: {patience})")
                break
        
        # ìµœê³  ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ¯ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ í‰ê°€")
        best_model_path = os.path.join(self.checkpoint_dir, 'best_model.pth')
        self.evaluate_model(test_loader, best_model_path)
        
        # í•™ìŠµ ê¸°ë¡ ì €ì¥
        history_path = os.path.join(self.result_dir, 'training_history.json')
        with open(history_path, 'w') as f:
            json.dump(train_history, f, indent=2)
        
        print(f"âœ… í•™ìŠµ ì™„ë£Œ! ê²°ê³¼ í´ë”: {self.result_dir}")

    def evaluate_model(self, test_loader: DataLoader, model_path: str):
        """ëª¨ë¸ í‰ê°€"""
        # ëª¨ë¸ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=self.device)
        
        model = MoriGirlVectorClassifier()
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()
        
        all_preds = []
        all_labels = []
        all_probs = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="í…ŒìŠ¤íŠ¸ í‰ê°€"):
                vectors = batch['vector'].to(self.device)
                labels = batch['label'].to(self.device)
                
                outputs = model(vectors)
                probs = outputs.cpu().numpy().flatten()
                preds = (probs > 0.5).astype(int)
                
                all_probs.extend(probs)
                all_preds.extend(preds)
                all_labels.extend(labels.cpu().numpy())
        
        # ìµœì¢… ì„±ëŠ¥ ì¶œë ¥
        accuracy = accuracy_score(all_labels, all_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels, all_preds, average='binary', zero_division=0
        )
        auc = roc_auc_score(all_labels, all_probs)
        
        print(f"\nğŸ“Š ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:")
        print(f"  - ì •í™•ë„: {accuracy:.4f}")
        print(f"  - ì •ë°€ë„: {precision:.4f}")
        print(f"  - ì¬í˜„ìœ¨: {recall:.4f}")
        print(f"  - F1 ì ìˆ˜: {f1:.4f}")
        print(f"  - AUC: {auc:.4f}")
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print(f"\nğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        print(classification_report(all_labels, all_preds, target_names=['ë¹„ëª¨ë¦¬ê±¸', 'ëª¨ë¦¬ê±¸']))

def main():
    parser = argparse.ArgumentParser(description='ëª¨ë¦¬ê±¸ ë²¡í„° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--data-path', default='data/morigirl_50', help='í•™ìŠµ ë°ì´í„° ê²½ë¡œ (ì˜ˆ: data/morigirl_50)')
    parser.add_argument('--epochs', type=int, default=50, help='í•™ìŠµ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--batch-size', type=int, default=32, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--learning-rate', type=float, default=0.001, help='í•™ìŠµë¥ ')
    parser.add_argument('--experiment-name', help='ì‹¤í—˜ ì´ë¦„')
    
    args = parser.parse_args()
    
    # í•™ìŠµ ì‹œì‘
    trainer = MoriGirlTrainer(
        data_path=args.data_path,
        experiment_name=args.experiment_name
    )
    
    trainer.train(
        epochs=args.epochs,
        learning_rate=args.learning_rate
    )

if __name__ == "__main__":
    main() 