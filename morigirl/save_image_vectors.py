#!/usr/bin/env python3
# save_image_vectors.py

import os
import sys
import json
import requests
import numpy as np
from PIL import Image
import torch
import torchvision.transforms as transforms
from torchvision.models import efficientnet_b0
from tqdm import tqdm
import io
import hashlib
from uuid import UUID
from typing import List, Dict, Any, Tuple
import sys
sys.path.append('..')
from database import DatabaseManager

class ImageVectorExtractor:
    """ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ DBì— ì €ì¥"""
    
    def __init__(self, config_path: str = "config.json"):
        with open(config_path, 'r') as f:
            self.config = json.load(f)
        
        # ë°ì´í„° ì €ì¥ í´ë”ëŠ” ë‚˜ì¤‘ì— max_productsì— ë”°ë¼ ì„¤ì •
        self.output_dir = None
        
        self.db_manager = DatabaseManager()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # EfficientNet ëª¨ë¸ ë¡œë“œ (íŠ¹ì§• ì¶”ì¶œìš©)
        self.model = efficientnet_b0(pretrained=True)
        self.model.classifier = torch.nn.Identity()  # ë¶„ë¥˜ì¸µ ì œê±°, íŠ¹ì§•ë§Œ ì¶”ì¶œ
        self.model.eval()
        self.model.to(self.device)
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        print(f"âœ… ì´ë¯¸ì§€ ë²¡í„° ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  - ë””ë°”ì´ìŠ¤: {self.device}")

    def _get_output_dir(self, max_products_per_type: int) -> str:
        """configì—ì„œ ì¶œë ¥ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        data_config = self.config["data"]
        data_paths = data_config.get("data_paths", {})
        
        # 1. base_data_dir ì‚¬ìš© (ìë™ ê²½ë¡œ ìƒì„±)
        if data_paths.get("auto_generate_path", True):
            base_path = data_paths.get("base_data_dir", "../data/morigirl_{max_products}")
            final_path = base_path.format(max_products=max_products_per_type)
            print(f"ğŸ“ ìë™ ìƒì„± ì¶œë ¥ ê²½ë¡œ: {final_path}")
            return final_path
        
        # 2. ê¸°ë³¸ê°’
        default_path = f"../data/morigirl_{max_products_per_type}"
        print(f"ğŸ“ ê¸°ë³¸ ì¶œë ¥ ê²½ë¡œ: {default_path}")
        return default_path

    def uuid_to_bigint(self, uuid_val) -> int:
        """UUIDë¥¼ BIGINTë¡œ ë³€í™˜"""
        if isinstance(uuid_val, str):
            uuid_val = UUID(uuid_val)
        elif isinstance(uuid_val, UUID):
            pass
        else:
            # ì´ë¯¸ intì¸ ê²½ìš°
            return int(uuid_val)
        
        # UUIDë¥¼ bytesë¡œ ë³€í™˜ í›„ í•´ì‹œí•˜ì—¬ 64bit ì •ìˆ˜ë¡œ ë³€í™˜
        uuid_bytes = uuid_val.bytes
        hash_bytes = hashlib.sha256(uuid_bytes).digest()
        # ì²« 8ë°”ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ 64bit ì •ìˆ˜ ìƒì„±
        return int.from_bytes(hash_bytes[:8], byteorder='big', signed=True)

    def download_image(self, url: str, timeout: int = 10) -> Image.Image:
        """URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        try:
            response = requests.get(url, timeout=timeout)
            response.raise_for_status()
            
            image = Image.open(io.BytesIO(response.content))
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            return image
        except Exception as e:
            raise ValueError(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

    def extract_features(self, image: Image.Image) -> np.ndarray:
        """ì´ë¯¸ì§€ì—ì„œ 1024ì°¨ì› íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
        try:
            # ì „ì²˜ë¦¬
            input_tensor = self.transform(image).unsqueeze(0).to(self.device)
            
            # íŠ¹ì§• ì¶”ì¶œ
            with torch.no_grad():
                features = self.model(input_tensor)
                features = features.cpu().numpy().flatten()
            
            # L2 ì •ê·œí™”
            features = features / np.linalg.norm(features)
            
            return features
        except Exception as e:
            raise ValueError(f"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    def get_morigirl_products(self, limit: int = 1000) -> List[Dict]:
        """ëª¨ë¦¬ê±¸ ìŠ¤íƒ€ì¼ ìƒí’ˆ ì¡°íšŒ (styles_id = 9)"""
        mysql_session = self.db_manager.mysql.Session()
        try:
            from sqlalchemy import text
            
            sql = text("""
                SELECT DISTINCT 
                    p.id,
                    p.status,
                    p.amount,
                    p.views,
                    p.impressions,
                    p.primary_category_id,
                    p.secondary_category_id
                FROM vingle.product p
                JOIN vingle.product_styles ps ON p.id = ps.product_id
                WHERE ps.styles_id = '9'
                  AND p.status IN ('SALE', 'SOLD_OUT')
                  AND p.amount IS NOT NULL
                  AND p.views IS NOT NULL
                  AND p.impressions IS NOT NULL
                  AND p.impressions > 0
                ORDER BY RAND()
                LIMIT :limit
            """)
            
            result = mysql_session.execute(sql, {"limit": limit})
            
            products = []
            for row in result.fetchall():
                products.append({
                    'product_id': row[0],
                    'status': row[1],
                    'amount': row[2],
                    'views': row[3],
                    'impressions': row[4],
                    'primary_category_id': row[5],
                    'secondary_category_id': row[6],
                    'is_morigirl': 1  # ëª¨ë¦¬ê±¸
                })
            
            print(f"ğŸ¨ ëª¨ë¦¬ê±¸ ìŠ¤íƒ€ì¼ ìƒí’ˆ {len(products)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return products
            
        finally:
            mysql_session.close()

    def get_non_morigirl_products(self, limit: int = 1000) -> List[Dict]:
        """ë¹„ëª¨ë¦¬ê±¸ ìƒí’ˆ ì¡°íšŒ (styles_id != 9)"""
        mysql_session = self.db_manager.mysql.Session()
        try:
            from sqlalchemy import text
            
            sql = text("""
                SELECT DISTINCT 
                    p.id,
                    p.status,
                    p.amount,
                    p.views,
                    p.impressions,
                    p.primary_category_id,
                    p.secondary_category_id
                FROM vingle.product p
                WHERE p.id NOT IN (
                    SELECT DISTINCT product_id 
                    FROM vingle.product_styles 
                    WHERE styles_id = '9'
                )
                  AND p.status IN ('SALE', 'SOLD_OUT')
                  AND p.amount IS NOT NULL
                  AND p.views IS NOT NULL
                  AND p.impressions IS NOT NULL
                  AND p.impressions > 0
                ORDER BY RAND()
                LIMIT :limit
            """)
            
            result = mysql_session.execute(sql, {"limit": limit})
            
            products = []
            for row in result.fetchall():
                products.append({
                    'product_id': row[0],
                    'status': row[1],
                    'amount': row[2],
                    'views': row[3],
                    'impressions': row[4],
                    'primary_category_id': row[5],
                    'secondary_category_id': row[6],
                    'is_morigirl': 0  # ë¹„ëª¨ë¦¬ê±¸
                })
            
            print(f"ğŸ“¦ ë¹„ëª¨ë¦¬ê±¸ ìƒí’ˆ {len(products)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return products
            
        finally:
            mysql_session.close()

    def get_product_vectors(self, product_ids: List[int]) -> Dict[int, List[float]]:
        """Vector DBì—ì„œ ìƒí’ˆ ë²¡í„° ì¡°íšŒ"""
        if not product_ids:
            return {}
            
        vector_session = self.db_manager.vector_db.Session()
        try:
            from sqlalchemy import text
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            batch_size = 1000
            all_vectors = {}
            
            for i in range(0, len(product_ids), batch_size):
                batch_ids = product_ids[i:i + batch_size]
                
                # product_idë¡œ ì§ì ‘ ë¹„êµ
                placeholders = ','.join([str(batch_id) for batch_id in batch_ids])
                sql = text(f"""
                    SELECT product_id, vector
                    FROM product_image_vector
                    WHERE product_id IN ({placeholders})
                      AND vector IS NOT NULL
                """)
                
                result = vector_session.execute(sql)
                
                for product_id, vector_str in result.fetchall():
                    if vector_str:
                        # "[1.0,2.0,3.0,...]" í˜•ì‹ì„ íŒŒì‹±
                        if isinstance(vector_str, str):
                            vector_str = vector_str.strip('[]')
                            vector = [float(x) for x in vector_str.split(',')]
                        else:
                            # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ë‚˜ ë°°ì—´ì¸ ê²½ìš°
                            vector = list(vector_str)
                        all_vectors[int(product_id)] = vector
            
            return all_vectors
            
        finally:
            vector_session.close()

    def calculate_sales_score(self, status: str, views: int, impressions: int) -> float:
        """íŒë§¤ ì ìˆ˜ ê³„ì‚° (0~1 ì‚¬ì´ ê°’)"""
        if status == 'SOLD_OUT':
            return 1.0
        
        # views / impressions ë¹„ìœ¨ë¡œ ê³„ì‚°
        if impressions > 0:
            ratio = min(views / impressions, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
            return ratio
        else:
            return 0.0

    def save_training_data_split(self, products_data: List[Dict[str, Any]], data_type: str):
        """í•™ìŠµìš© ë°ì´í„°ë¥¼ train/testë¡œ ë¶„í• í•˜ì—¬ ì €ì¥"""
        if not products_data:
            return 0, 0
        
        # ì„¸ì…˜ë³„ í´ë” ìƒì„±
        os.makedirs(self.output_dir, exist_ok=True)
        
        try:
            # ë°ì´í„° ì •ë¦¬
            training_data = []
            for item in products_data:
                training_item = {
                    'product_id': item['product_id'],
                    'price': item['amount'],
                    'vector': item['vector'],
                    'first_category': item['primary_category_id'],
                    'second_category': item['secondary_category_id'],
                    'is_morigirl': item['is_morigirl'],
                    'sales_score': item['sales_score']
                }
                training_data.append(training_item)
            
            # configì—ì„œ train/test ë¹„ìœ¨ ì½ê¸°
            train_ratio = self.config["data"]["train_test_split"]  # 0.8
            
            # train/test ë¶„í• 
            from sklearn.model_selection import train_test_split
            train_data, test_data = train_test_split(
                training_data, 
                test_size=1-train_ratio, 
                random_state=42,
                stratify=[item['is_morigirl'] for item in training_data]  # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
            )
            
            # train íŒŒì¼ ì €ì¥
            train_filename = f"{self.output_dir}/{data_type}_train.npy"
            np.save(train_filename, train_data)
            print(f"ğŸ“ {data_type} train ë°ì´í„° ì €ì¥: {train_filename} ({len(train_data)}ê°œ)")
            
            # test íŒŒì¼ ì €ì¥
            test_filename = f"{self.output_dir}/{data_type}_test.npy"
            np.save(test_filename, test_data)
            print(f"ğŸ“ {data_type} test ë°ì´í„° ì €ì¥: {test_filename} ({len(test_data)}ê°œ)")
            
            return len(train_data), len(test_data)
            
        except Exception as e:
            print(f"âŒ {data_type} ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0, 0

    def process_training_data(self, max_products_per_type: int = 5000):
        """í•™ìŠµìš© ë°ì´í„° ì²˜ë¦¬ - ëª¨ë¦¬ê±¸/ë¹„ëª¨ë¦¬ê±¸ ë¶„ë¦¬"""
        
        # ë°ì´í„° í´ë” ì„¤ì • (configì—ì„œ ì½ê¸°)
        self.output_dir = self._get_output_dir(max_products_per_type)
        
        print(f"ğŸš€ í•™ìŠµìš© ë°ì´í„° ìƒì„± ì‹œì‘")
        print(f"  - ëª¨ë¦¬ê±¸ ìµœëŒ€: {max_products_per_type:,}ê°œ")
        print(f"  - ë¹„ëª¨ë¦¬ê±¸ ìµœëŒ€: {max_products_per_type:,}ê°œ")
        print(f"  - ì €ì¥ í´ë”: {self.output_dir}")
        
        # 1. ëª¨ë¦¬ê±¸ ë°ì´í„° ì²˜ë¦¬
        print(f"\n=== ëª¨ë¦¬ê±¸ ë°ì´í„° ì²˜ë¦¬ ===")
        morigirl_products = self.get_morigirl_products(max_products_per_type)
        morigirl_train, morigirl_test = self._process_product_batch(morigirl_products, "morigirl")
        
        # 2. ë¹„ëª¨ë¦¬ê±¸ ë°ì´í„° ì²˜ë¦¬
        print(f"\n=== ë¹„ëª¨ë¦¬ê±¸ ë°ì´í„° ì²˜ë¦¬ ===")
        non_morigirl_products = self.get_non_morigirl_products(max_products_per_type)
        non_morigirl_train, non_morigirl_test = self._process_product_batch(non_morigirl_products, "non_morigirl")
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ‰ í•™ìŠµìš© ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        print(f"  - ëª¨ë¦¬ê±¸ train: {morigirl_train}ê°œ, test: {morigirl_test}ê°œ (ì´ {morigirl_train + morigirl_test}ê°œ)")
        print(f"  - ë¹„ëª¨ë¦¬ê±¸ train: {non_morigirl_train}ê°œ, test: {non_morigirl_test}ê°œ (ì´ {non_morigirl_train + non_morigirl_test}ê°œ)")
        print(f"  - ì´ train: {morigirl_train + non_morigirl_train}ê°œ")
        print(f"  - ì´ test: {morigirl_test + non_morigirl_test}ê°œ")
        print(f"  - ì „ì²´ ì´í•©: {morigirl_train + morigirl_test + non_morigirl_train + non_morigirl_test}ê°œ")
        
        # ë°ì´í„° ì •ë³´ JSON íŒŒì¼ ìƒì„±
        data_info = {
            "dataset_info": {
                "folder_name": self.output_dir,
                "creation_time": str(np.datetime64('now')),
                "max_products_per_type": max_products_per_type,
                "train_ratio": self.config["data"]["train_test_split"],
                "total_count": morigirl_train + morigirl_test + non_morigirl_train + non_morigirl_test
            },
            "file_counts": {
                "morigirl_train.npy": morigirl_train,
                "morigirl_test.npy": morigirl_test,
                "non_morigirl_train.npy": non_morigirl_train,
                "non_morigirl_test.npy": non_morigirl_test
            },
            "summary": {
                "total_train": morigirl_train + non_morigirl_train,
                "total_test": morigirl_test + non_morigirl_test,
                "morigirl_total": morigirl_train + morigirl_test,
                "non_morigirl_total": non_morigirl_train + non_morigirl_test,
                "train_ratio_actual": (morigirl_train + non_morigirl_train) / (morigirl_train + morigirl_test + non_morigirl_train + non_morigirl_test),
                "morigirl_ratio": (morigirl_train + morigirl_test) / (morigirl_train + morigirl_test + non_morigirl_train + non_morigirl_test)
            },
            "files_created": [
                f"{self.output_dir}/morigirl_train.npy",
                f"{self.output_dir}/morigirl_test.npy", 
                f"{self.output_dir}/non_morigirl_train.npy",
                f"{self.output_dir}/non_morigirl_test.npy"
            ]
        }
        
        # JSON íŒŒì¼ ì €ì¥
        data_info_file = f"{self.output_dir}/data_info.json"
        with open(data_info_file, 'w', encoding='utf-8') as f:
            json.dump(data_info, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ ë°ì´í„° ì •ë³´ ì €ì¥: {data_info_file}")
        
        # í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ ë³€ìˆ˜ëª… ìœ ì§€
        result_data = data_info
        
        # try:
        #     result_file = f"{self.output_dir}/training_data_result.json"
        #     with open(result_file, 'w', encoding='utf-8') as f:
        #         json.dump(result_data, f, ensure_ascii=False, indent=2)
        #     print(f"ğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥: {result_file}")
        # except Exception as e:
        #     print(f"âš ï¸ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # í´ë” ê²½ë¡œ ë°˜í™˜
        return self.output_dir

    def _process_product_batch(self, products: List[Dict], data_type: str) -> Tuple[int, int]:
        """ìƒí’ˆ ë°°ì¹˜ ì²˜ë¦¬ (train/test ë¶„í• )"""
        if not products:
            return 0, 0
        
        # ë²¡í„° ì¡°íšŒ
        product_ids = [p['product_id'] for p in products]
        product_vectors = self.get_product_vectors(product_ids)
        
        # ë²¡í„°ê°€ ìˆëŠ” ìƒí’ˆë§Œ í•„í„°ë§
        valid_products = []
        for product in tqdm(products, desc=f"{data_type} ë°ì´í„° ì²˜ë¦¬"):
            product_id = product['product_id']
            
            if product_id in product_vectors:
                # íŒë§¤ ì ìˆ˜ ê³„ì‚°
                sales_score = self.calculate_sales_score(
                    product['status'], 
                    product['views'], 
                    product['impressions']
                )
                
                # ìµœì¢… ë°ì´í„° êµ¬ì„±
                product_data = {
                    'product_id': product_id,
                    'amount': product['amount'],
                    'vector': product_vectors[product_id],
                    'primary_category_id': product['primary_category_id'],
                    'secondary_category_id': product['secondary_category_id'],
                    'is_morigirl': product['is_morigirl'],
                    'sales_score': sales_score
                }
                
                valid_products.append(product_data)
        
        print(f"ğŸ’¡ {data_type}: ë²¡í„° ìˆëŠ” ìƒí’ˆ {len(valid_products)}ê°œ / ì „ì²´ {len(products)}ê°œ")
        
        # íŒŒì¼ ì €ì¥ (train/test ë¶„í• )
        if valid_products:
            train_count, test_count = self.save_training_data_split(valid_products, data_type)
            return train_count, test_count
        
        return 0, 0

    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'db_manager'):
            self.db_manager.dispose_all()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ëª¨ë¦¬ê±¸ í•™ìŠµìš© ë°ì´í„° ìƒì„±')
    parser.add_argument('--config-path', default='config.json', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--max-products', type=int, default=None, 
                       help='ê° íƒ€ì…ë³„ ìµœëŒ€ ìƒí’ˆ ìˆ˜ (ì„¤ì • íŒŒì¼ ìš°ì„ )')
    
    args = parser.parse_args()
    
    try:
        # config.jsonì—ì„œ max_products ì½ê¸°
        if args.max_products is None:
            try:
                with open(args.config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                max_products = config["data"]["max_products_per_type"]
                print(f"âœ… config.jsonì—ì„œ max_products ë¡œë“œ: {max_products}")
            except Exception as e:
                print(f"âš ï¸  config.json ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                max_products = 5000
        else:
            max_products = args.max_products
            print(f"âœ… ì»¤ë§¨ë“œë¼ì¸ì—ì„œ max_products ì„¤ì •: {max_products}")
        
        extractor = ImageVectorExtractor()
        
        print(f"ğŸš€ ëª¨ë¦¬ê±¸ í•™ìŠµìš© ë°ì´í„° ìƒì„± ì‹œì‘")
        print(f"  - ê° íƒ€ì…ë³„ ìµœëŒ€: {max_products:,}ê°œ")
        
        # í•™ìŠµìš© ë°ì´í„° ìƒì„±
        data_folder = extractor.process_training_data(
            max_products_per_type=max_products
        )
        
        print(f"\nğŸ‰ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“ ë°ì´í„° í´ë”: {data_folder}")
        print(f"ğŸ’¡ í•™ìŠµ ì‹œ --data-path {data_folder} ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        try:
            extractor.close()
        except:
            pass

if __name__ == "__main__":
    main() 