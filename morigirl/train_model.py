#!/usr/bin/env python3
# train_model.py

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from datetime import datetime
from tqdm import tqdm
from typing import Dict, Tuple
import argparse
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, classification_report

# wandb import
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸  wandbê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install wandb ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

# ë¡œì»¬ ëª¨ë“ˆ
import sys
sys.path.append('..')
from prepare_training_data import MorigirlDataProcessor, MorigirlDataset
from morigirl.morigirl_model import MoriGirlVectorClassifier, get_model_info

class MoriGirlTrainer:
    """ëª¨ë¦¬ê±¸ ë²¡í„° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 config_path: str = "config.json",
                 data_path: str = None,
                 experiment_name: str = None):
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        self.config = self.load_config(config_path)
        
        # ë°ì´í„° ê²½ë¡œ ì„¤ì • (ìš°ì„ ìˆœìœ„: íŒŒë¼ë¯¸í„° > config > ê¸°ë³¸ê°’)
        if data_path is None:
            self.data_path = self._get_data_path()
        else:
            self.data_path = data_path
            
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ì‹¤í—˜ ì´ë¦„ê³¼ ê²°ê³¼ í´ë” ì„¤ì •
        self.experiment_name = self._get_experiment_name(experiment_name)
        self.result_dir = self._get_result_dir()
        self.checkpoint_dir = f"{self.result_dir}/checkpoints"
        
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # config.jsonì„ ì‹¤í—˜ í´ë”ì— ë³µì‚¬í•˜ì—¬ ì €ì¥
        self._save_experiment_config(config_path)
        
        # wandb ì´ˆê¸°í™”
        self.use_wandb = WANDB_AVAILABLE and self.config["wandb"]["enabled"]
        if self.use_wandb:
            self.init_wandb()
        
        print(f"ğŸš€ ëª¨ë¦¬ê±¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print(f"  - ì‹¤í—˜ëª…: {self.experiment_name}")
        print(f"  - ë°ì´í„° ê²½ë¡œ: {self.data_path}")
        print(f"  - ê²°ê³¼ í´ë”: {self.result_dir}")
        print(f"  - ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"  - wandb: {'í™œì„±í™”' if self.use_wandb else 'ë¹„í™œì„±í™”'}")

    def load_config(self, config_path: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
            return config
        except Exception as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì„¤ì • ë°˜í™˜
            return {
                "data": {"max_products_per_type": 5000, "train_test_split": 0.8, "val_split": 0.1},
                "model": {"input_vector_dim": 1024, "hidden_dim": 128, "dropout_rate": 0.1, 
                         "learning_rate": 1e-4, "weight_decay": 0.01, "batch_size": 64, 
                         "epochs": 50, "patience": 10},
                "wandb": {"enabled": False, "project": "morigirl-classification"}
            }

    def _get_data_path(self) -> str:
        """configì—ì„œ ë°ì´í„° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        data_config = self.config["data"]
        data_paths = data_config.get("data_paths", {})
        
        # 1. train_data_dirì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©
        if data_paths.get("train_data_dir"):
            print(f"ğŸ“ ì‚¬ìš©ì ì§€ì • train ë°ì´í„° ê²½ë¡œ: {data_paths['train_data_dir']}")
            return data_paths["train_data_dir"]
        
        # 2. base_data_dir ì‚¬ìš© (ìë™ ê²½ë¡œ ìƒì„±)
        if data_paths.get("auto_generate_path", True):
            max_products = data_config["max_products_per_type"]
            base_path = data_paths.get("base_data_dir", "../data/morigirl_{max_products}")
            final_path = base_path.format(max_products=max_products)
            print(f"ğŸ“ ìë™ ìƒì„± ë°ì´í„° ê²½ë¡œ: {final_path}")
            return final_path
        
        # 3. ê¸°ë³¸ê°’
        max_products = data_config["max_products_per_type"]
        default_path = f"../data/morigirl_{max_products}"
        print(f"ğŸ“ ê¸°ë³¸ ë°ì´í„° ê²½ë¡œ: {default_path}")
        return default_path

    def _get_experiment_name(self, experiment_name: str = None) -> str:
        """configì—ì„œ ì‹¤í—˜ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°"""
        data_config = self.config["data"]
        result_paths = data_config.get("result_paths", {})
        
        # 1. íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ ì‹¤í—˜ ì´ë¦„ ìš°ì„ 
        if experiment_name:
            print(f"ğŸ”¬ ì‚¬ìš©ì ì§€ì • ì‹¤í—˜ ì´ë¦„: {experiment_name}")
            return experiment_name
            
        # 2. configì—ì„œ ì‹¤í—˜ ì´ë¦„ ì‚¬ìš©
        if result_paths.get("experiment_name"):
            print(f"ğŸ”¬ config ì‹¤í—˜ ì´ë¦„: {result_paths['experiment_name']}")
            return result_paths["experiment_name"]
        
        # 3. ìë™ ìƒì„± (ì›”ì¼ì‹œë¶„_ëœë¤2ìë¦¬)
        import random
        date_str = datetime.now().strftime('%m%d%H%M')  # ì›”ì¼ì‹œë¶„
        random_num = random.randint(10, 99)  # ëœë¤ 2ìë¦¬
        auto_name = f"{date_str}_{random_num:02d}"
        print(f"ğŸ”¬ ìë™ ìƒì„± ì‹¤í—˜ ì´ë¦„: {auto_name}")
        return auto_name

    def _get_result_dir(self) -> str:
        """configì—ì„œ ê²°ê³¼ í´ë” ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        data_config = self.config["data"]
        result_paths = data_config.get("result_paths", {})
        
        # 1. ìë™ ê²½ë¡œ ìƒì„± (ê¸°ë³¸ê°’)
        if result_paths.get("auto_generate_result_path", True):
            base_result_dir = result_paths.get("base_result_dir", "result")
            final_path = f"{base_result_dir}/{self.experiment_name}"
            print(f"ğŸ“ ìë™ ìƒì„± ê²°ê³¼ ê²½ë¡œ: {final_path}")
            return final_path
        
        # 2. ê¸°ë³¸ê°’
        default_path = f"result/{self.experiment_name}"
        print(f"ğŸ“ ê¸°ë³¸ ê²°ê³¼ ê²½ë¡œ: {default_path}")
        return default_path

    def _save_experiment_config(self, config_path: str):
        """ì‹¤í—˜ ì„¤ì •ì„ ê²°ê³¼ í´ë”ì— ì €ì¥"""
        import shutil
        import json
        from datetime import datetime
        
        try:
            # ì›ë³¸ config ë³µì‚¬
            experiment_config_path = f"{self.result_dir}/config.json"
            shutil.copy2(config_path, experiment_config_path)
            
            # ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì €ì¥
            experiment_metadata = {
                "experiment_name": self.experiment_name,
                "start_time": datetime.now().isoformat(),
                "data_path": self.data_path,
                "result_dir": self.result_dir,
                "device": str(self.device),
                "wandb_enabled": self.use_wandb,
                "original_config_path": config_path
            }
            
            with open(f"{self.result_dir}/experiment_metadata.json", 'w', encoding='utf-8') as f:
                json.dump(experiment_metadata, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ì‹¤í—˜ ì„¤ì • ì €ì¥:")
            print(f"  - {experiment_config_path}")
            print(f"  - {self.result_dir}/experiment_metadata.json")
            
        except Exception as e:
            print(f"âš ï¸  ì‹¤í—˜ ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")

    def init_wandb(self):
        """wandb ì´ˆê¸°í™”"""
        try:
            wandb_config = self.config["wandb"]
            
            # wandb ì„¤ì •
            wandb.init(
                project=wandb_config["project"],
                entity=wandb_config.get("entity"),
                name=self.experiment_name,
                config={
                    "model": self.config["model"],
                    "data": self.config["data"],
                    "experiment_name": self.experiment_name,
                    "data_path": self.data_path
                }
            )
            
            print(f"âœ… wandb ì´ˆê¸°í™” ì™„ë£Œ: {wandb_config['project']}")
            
        except Exception as e:
            print(f"âš ï¸  wandb ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.use_wandb = False

    def setup_datasets(self) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """ë°ì´í„°ì…‹ ì„¤ì •"""
        print(f"\nğŸ“Š ë°ì´í„°ì…‹ ì„¤ì •")
        
        # configì—ì„œ ì„¤ì • ì½ê¸°
        data_config = self.config["data"]
        model_config = self.config["model"]
        
        test_size = 1 - data_config["train_test_split"]
        val_size = data_config["val_split"] 
        batch_size = model_config["batch_size"]
        
        # ë°ì´í„° ì²˜ë¦¬ê¸° ìƒì„± ë° ë¡œë”© (train íŒŒì¼ë§Œ)
        train_processor = MorigirlDataProcessor(self.data_path)
        if not train_processor.load_npy_files(split_type="train"):
            raise RuntimeError("Train ë°ì´í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # Test ë°ì´í„° ë¡œë”©
        test_processor = MorigirlDataProcessor(self.data_path)
        if not test_processor.load_npy_files(split_type="test"):
            raise RuntimeError("Test ë°ì´í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # Test ë°ì´í„°ì…‹ ìƒì„± (ë¯¸ë¦¬ ë¶„í• ëœ test íŒŒì¼ ì‚¬ìš©)
        test_dataset = MorigirlDataset(
            test_processor.vectors, 
            test_processor.labels, 
            test_processor.product_ids
        )
        
        # Trainì—ì„œ Validation ë¶„í• 
        # val_sizeëŠ” ì „ì²´ ë°ì´í„° ê¸°ì¤€ì´ë¯€ë¡œ, train ë°ì´í„° ë‚´ì—ì„œì˜ ë¹„ìœ¨ë¡œ ì¡°ì •
        # ì˜ˆ: ì „ì²´ì—ì„œ train 80%, val 10%, test 20%ë¼ë©´
        # train íŒŒì¼ì—ì„œ valì„ ë¶„í• í•  ë•ŒëŠ” 10/80 = 0.125 ë¹„ìœ¨ë¡œ ë¶„í• 
        val_size_adjusted = val_size / data_config["train_test_split"]
        
        train_vectors = train_processor.vectors
        train_labels = train_processor.labels
        train_ids = train_processor.product_ids
        
        from sklearn.model_selection import train_test_split
        
        # ì—°ì†ê°’ ë¼ë²¨ì„ ì´ì§„ê°’ìœ¼ë¡œ ë³€í™˜ (stratifyìš©)
        # 0.5 ê¸°ì¤€ìœ¼ë¡œ ì´ì§„í™”: 0.5 ì´ìƒì´ë©´ 1(ëª¨ë¦¬ê±¸), ë¯¸ë§Œì´ë©´ 0(ë¹„ëª¨ë¦¬ê±¸)
        binary_labels_for_stratify = (train_labels >= 0.5).astype(int)
        
        X_train, X_val, y_train, y_val, ids_train, ids_val = train_test_split(
            train_vectors, train_labels, train_ids,
            test_size=val_size_adjusted,
            random_state=42,
            stratify=binary_labels_for_stratify  # ì´ì§„ ë¼ë²¨ë¡œ stratify
        )
        
        # ìƒˆë¡œìš´ Dataset ê°ì²´ ìƒì„±
        train_dataset = MorigirlDataset(X_train, y_train, ids_train)
        val_dataset = MorigirlDataset(X_val, y_val, ids_val)
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• í•´ê²°)
        pos_count = torch.sum(train_dataset.labels).item()
        neg_count = len(train_dataset) - pos_count
        self.pos_weight = torch.tensor(neg_count / pos_count).to(self.device)
        print(f"ğŸ“Š í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {self.pos_weight.item():.2f}")
        
        # wandbì— ë°ì´í„° ì •ë³´ ë¡œê·¸
        if self.use_wandb:
            total_samples = len(train_dataset) + len(val_dataset) + len(test_dataset)
            train_pos = torch.sum(train_dataset.labels).item()
            val_pos = torch.sum(val_dataset.labels).item()
            test_pos = torch.sum(test_dataset.labels).item()
            
            wandb.log({
                "data/train_size": len(train_dataset),
                "data/val_size": len(val_dataset), 
                "data/test_size": len(test_dataset),
                "data/total_size": total_samples,
                "data/pos_weight": self.pos_weight.item(),
                "data/train_pos_ratio": train_pos / len(train_dataset),
                "data/val_pos_ratio": val_pos / len(val_dataset),
                "data/test_pos_ratio": test_pos / len(test_dataset),
                "data/train_pos_count": train_pos,
                "data/val_pos_count": val_pos,
                "data/test_pos_count": test_pos,
                "data/vector_dim": train_dataset.vectors.shape[1],
                "data/batch_size": batch_size
            })
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        from torch.utils.data import DataLoader
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        print(f"ğŸ“¦ DataLoader ìƒì„± ì™„ë£Œ:")
        print(f"  - Train batches: {len(train_loader)}")
        print(f"  - Val batches: {len(val_loader)}")
        print(f"  - Test batches: {len(test_loader)}")
        print(f"  - Batch size: {batch_size}")
        
        return train_loader, val_loader, test_loader

    def setup_model(self) -> nn.Module:
        """ëª¨ë¸ ì„¤ì •"""
        print(f"\nğŸ§  ëª¨ë¸ ì„¤ì •")
        
        # configì—ì„œ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì½ê¸°
        model_config = self.config["model"]
        model_kwargs = {
            "input_dim": model_config["input_vector_dim"],
            "hidden_dim": model_config["hidden_dim"],
            "hidden_dim2": model_config["hidden_dim2"],
            "dropout_rate": model_config["dropout_rate"]
        }
        
        model = MoriGirlVectorClassifier(**model_kwargs)
        model.to(self.device)
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        get_model_info(model)
        
        # wandbì— ëª¨ë¸ ì •ë³´ ë¡œê·¸
        if self.use_wandb:
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            model_size_mb = total_params * 4 / (1024 * 1024)  # float32 ê¸°ì¤€
            
            wandb.log({
                "model/total_params": total_params,
                "model/trainable_params": trainable_params,
                "model/model_size_mb": model_size_mb,
                "model/input_dim": model_config["input_vector_dim"],
                "model/hidden_dim": model_config["hidden_dim"],
                "model/hidden_dim2": model_config["hidden_dim2"],
                "model/dropout_rate": model_config["dropout_rate"],
                "model/architecture": "2-layer-mlp"
            })
            
            # ëª¨ë¸ ê°ì‹œ (gradients, parameters)
            if self.config["wandb"]["watch_model"]:
                wandb.watch(model, log="all", log_freq=self.config["wandb"]["log_frequency"])
        
        return model

    def train_epoch(self, model: nn.Module, train_loader: DataLoader, 
                   criterion: nn.Module, optimizer: optim.Optimizer, epoch: int, show_progress: bool = True) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        model.train()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        all_probs = []
        
        progress_bar = tqdm(train_loader, desc="í•™ìŠµ", disable=not show_progress)
        
        # ë°°ì¹˜ ë‹¨ìœ„ ë¡œê¹…ì„ ìœ„í•œ ë³€ìˆ˜
        log_frequency = self.config["wandb"]["log_frequency"]
        
        for batch_idx, batch in enumerate(progress_bar):
            vectors = batch['vector'].to(self.device)
            labels = batch['label'].to(self.device).unsqueeze(1)  # (batch_size, 1)
            
            # Forward pass
            outputs = model(vectors)  # ì´ë¯¸ sigmoid ì ìš©ë¨
            loss = criterion(outputs, labels)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            
            # Gradient norm ê³„ì‚°
            grad_norm = 0.0
            for param in model.parameters():
                if param.grad is not None:
                    grad_norm += param.grad.data.norm(2).item() ** 2
            grad_norm = grad_norm ** 0.5
            
            optimizer.step()
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            total_loss += loss.item()
            probs = outputs.detach().cpu().numpy()
            preds = (probs > 0.5).astype(int)
            
            all_probs.extend(probs.flatten())
            all_preds.extend(preds.flatten().astype(int))  # ëª…ì‹œì ìœ¼ë¡œ int íƒ€ì… ë³´ì¥
            all_labels.extend(labels.cpu().numpy().flatten())
            
            # Progress bar ì—…ë°ì´íŠ¸
            avg_loss = total_loss / (batch_idx + 1)
            progress_bar.set_postfix({'Loss': f'{avg_loss:.4f}', 'GradNorm': f'{grad_norm:.4f}'})
            
            # ë°°ì¹˜ ë‹¨ìœ„ ë¡œê¹… ì œê±° - ì—í¬í¬ ë‹¨ìœ„ë¡œë§Œ ë¡œê¹…
        
        # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
        # ì—°ì†ê°’ ë¼ë²¨ì„ ì´ì§„ê°’ìœ¼ë¡œ ë³€í™˜ (ë©”íŠ¸ë¦­ ê³„ì‚°ìš©)
        binary_labels = (np.array(all_labels) >= 0.5).astype(int)
        binary_preds = np.array(all_preds).astype(int)  # ì˜ˆì¸¡ê°’ë„ ëª…ì‹œì ìœ¼ë¡œ ì´ì§„ê°’ìœ¼ë¡œ ë³€í™˜
        
        accuracy = accuracy_score(binary_labels, binary_preds)
        try:
            auc = roc_auc_score(binary_labels, all_probs)
        except:
            auc = 0.0
        
        return {
            'loss': total_loss / len(train_loader),
            'accuracy': accuracy,
            'auc': auc,
            'grad_norm': grad_norm
        }

    def validate_epoch(self, model: nn.Module, val_loader: DataLoader, 
                      criterion: nn.Module, show_progress: bool = True) -> Dict[str, float]:
        """í•œ ì—í¬í¬ ê²€ì¦"""
        model.eval()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        all_probs = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="ê²€ì¦", disable=not show_progress):
                vectors = batch['vector'].to(self.device)
                labels = batch['label'].to(self.device).unsqueeze(1)
                
                outputs = model(vectors)
                loss = criterion(outputs, labels)
                
                total_loss += loss.item()
                probs = outputs.cpu().numpy()
                preds = (probs > 0.5).astype(int)
                
                all_probs.extend(probs.flatten())
                all_preds.extend(preds.flatten().astype(int))  # ëª…ì‹œì ìœ¼ë¡œ int íƒ€ì… ë³´ì¥
                all_labels.extend(labels.cpu().numpy().flatten())
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        # ì—°ì†ê°’ ë¼ë²¨ì„ ì´ì§„ê°’ìœ¼ë¡œ ë³€í™˜ (ë©”íŠ¸ë¦­ ê³„ì‚°ìš©)
        binary_labels = (np.array(all_labels) >= 0.5).astype(int)
        binary_preds = np.array(all_preds).astype(int)  # ì˜ˆì¸¡ê°’ë„ ëª…ì‹œì ìœ¼ë¡œ ì´ì§„ê°’ìœ¼ë¡œ ë³€í™˜
        
        accuracy = accuracy_score(binary_labels, binary_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            binary_labels, binary_preds, average='binary', zero_division=0
        )
        try:
            auc = roc_auc_score(binary_labels, all_probs)
        except:
            auc = 0.0
        
        return {
            'loss': total_loss / len(val_loader),
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'num_samples': len(all_labels),
            'pos_samples': sum(all_labels),
            'neg_samples': len(all_labels) - sum(all_labels)
        }

    def save_checkpoint(self, model: nn.Module, optimizer: optim.Optimizer, 
                       epoch: int, metrics: Dict[str, float], is_best: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë§Œ)"""
        if is_best:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'metrics': metrics,
                'experiment_name': self.experiment_name
            }
            
            best_path = os.path.join(self.checkpoint_dir, 'best_model.pth')
            torch.save(checkpoint, best_path)

    def train(self):
        """ëª¨ë¸ í•™ìŠµ (config.json ê¸°ë°˜)"""
        
        # configì—ì„œ í•™ìŠµ íŒŒë¼ë¯¸í„° ì½ê¸°
        model_config = self.config["model"]
        epochs = model_config["epochs"]
        learning_rate = model_config["learning_rate"]
        weight_decay = model_config["weight_decay"]
        patience = model_config["patience"]
        min_delta = model_config.get("min_delta", 0.001)  # ìµœì†Œ ì„±ëŠ¥ í–¥ìƒ ì„ê³„ê°’
        early_stopping_enabled = model_config.get("early_stopping_enabled", True)  # Early stopping í™œì„±í™”
        
        # ë°ì´í„°ì…‹ ì„¤ì •
        train_loader, val_loader, test_loader = self.setup_datasets()
        
        # ëª¨ë¸ ì„¤ì •
        model = self.setup_model()
        
        # ì†ì‹¤í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        criterion = nn.BCELoss(weight=self.pos_weight if self.pos_weight.item() > 1 else None)
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)  # ìŠ¤ì¼€ì¤„ëŸ¬ë„ ë” ë¹ ë¥´ê²Œ
        
        # ì—„ê²©í•œ Early Stoppingì„ ìœ„í•œ ì¶”ì  ë³€ìˆ˜ë“¤
        best_val_acc = 0.0
        best_val_f1 = 0.0
        best_composite_score = 0.0  # accuracy + f1ì˜ ì¡°í•© ì ìˆ˜
        patience_counter = 0
        no_improvement_counter = 0  # ì—°ì† ì„±ëŠ¥ ì €í•˜ ì¶”ì 
        train_history = []
        
        # ì¶œë ¥ ê°„ê²© ì„¤ì • (ì „ì²´ ì—í¬í¬ì˜ 10ë¶„ì˜ 1)
        print_interval = max(1, epochs // 10)  # ìµœì†Œ 1 ì—í¬í¬ë§ˆë‹¤ëŠ” ì¶œë ¥
        print(f"\nğŸ¯ í•™ìŠµ ì‹œì‘ (ì´ {epochs} ì—í¬í¬, {print_interval} ì—í¬í¬ë§ˆë‹¤ ì¶œë ¥)")
        print(f"â° Early stopping: {'í™œì„±í™”' if early_stopping_enabled else 'ë¹„í™œì„±í™”'}")
        
        for epoch in range(epochs):
            # ì¶œë ¥ ì—¬ë¶€ ê²°ì • (ì²« ì—í¬í¬, ë§ˆì§€ë§‰ ì—í¬í¬, ë˜ëŠ” ì„¤ì •ëœ ê°„ê²©)
            should_print = (epoch == 0 or 
                          epoch == epochs - 1 or 
                          (epoch + 1) % print_interval == 0)
            
            if should_print:
                print(f"\n=== Epoch {epoch+1}/{epochs} ===")
            
            # í•™ìŠµ
            train_metrics = self.train_epoch(model, train_loader, criterion, optimizer, epoch, show_progress=should_print)
            
            # ê²€ì¦
            val_metrics = self.validate_epoch(model, val_loader, criterion, show_progress=should_print)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            current_lr = optimizer.param_groups[0]['lr']
            scheduler.step(val_metrics['accuracy'])
            
            # ê²°ê³¼ ì¶œë ¥ (ì¡°ê±´ë¶€)
            if should_print:
                print(f"í•™ìŠµ - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, AUC: {train_metrics['auc']:.4f}")
                print(f"ê²€ì¦ - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}, AUC: {val_metrics['auc']:.4f}")
            
            # ì—„ê²©í•œ ì„±ëŠ¥ í‰ê°€ (accuracy + f1 ì¡°í•© ì ìˆ˜)
            current_composite_score = (val_metrics['accuracy'] + val_metrics['f1']) / 2
            
            # wandb ë¡œê¹… (ì—í¬í¬ ë‹¨ìœ„) - ì •ìˆ˜ step ì‚¬ìš©
            if self.use_wandb:
                wandb.log({
                    "epoch": epoch + 1,
                    "train/loss": train_metrics['loss'],
                    "train/accuracy": train_metrics['accuracy'],
                    "train/auc": train_metrics['auc'],
                    "train/grad_norm": train_metrics['grad_norm'],
                    "val/loss": val_metrics['loss'],
                    "val/accuracy": val_metrics['accuracy'],
                    "val/precision": val_metrics['precision'],
                    "val/recall": val_metrics['recall'],
                    "val/f1": val_metrics['f1'],
                    "val/auc": val_metrics['auc'],
                    "learning_rate": current_lr,
                    "optimizer/lr": current_lr,
                    "metrics/loss_diff": train_metrics['loss'] - val_metrics['loss'],
                    "metrics/accuracy_diff": train_metrics['accuracy'] - val_metrics['accuracy'],
                    "metrics/composite_score": current_composite_score,
                    "patience/counter": patience_counter,
                    "patience/no_improvement": no_improvement_counter
                }, step=epoch + 1)
            
            # ì„±ëŠ¥ í–¥ìƒì´ ì¶©ë¶„í•œì§€ í™•ì¸ (min_delta ì„ê³„ê°’ ì ìš©)
            acc_improvement = val_metrics['accuracy'] - best_val_acc
            f1_improvement = val_metrics['f1'] - best_val_f1
            composite_improvement = current_composite_score - best_composite_score
            
            # ì—¬ëŸ¬ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•´ì•¼ "ì§„ì§œ ê°œì„ "ìœ¼ë¡œ ì¸ì •
            significant_acc_improvement = acc_improvement > min_delta
            significant_f1_improvement = f1_improvement > min_delta  
            significant_composite_improvement = composite_improvement > min_delta
            
            # ìµœê³  ì„±ëŠ¥ í™•ì¸ (ë” ì—„ê²©í•œ ì¡°ê±´)
            is_best = (significant_acc_improvement and significant_f1_improvement) or significant_composite_improvement
            
            if is_best:
                best_val_acc = val_metrics['accuracy']
                best_val_f1 = val_metrics['f1']
                best_composite_score = current_composite_score
                patience_counter = 0
                no_improvement_counter = 0
                
                if should_print:
                    print(f"ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! Acc: {best_val_acc:.4f} (+{acc_improvement:.4f}), F1: {best_val_f1:.4f} (+{f1_improvement:.4f})")
                
                # wandbì— ìµœê³  ì„±ëŠ¥ ê¸°ë¡
                if self.use_wandb:
                    wandb.log({
                        "best/epoch": epoch + 1,
                        "best/val_accuracy": best_val_acc,
                        "best/val_f1": best_val_f1,
                        "best/val_precision": val_metrics['precision'],
                        "best/val_recall": val_metrics['recall'],
                        "best/val_auc": val_metrics['auc'],
                        "best/val_loss": val_metrics['loss'],
                        "best/composite_score": best_composite_score,
                        "best/train_accuracy": train_metrics['accuracy'],
                        "best/train_loss": train_metrics['loss'],
                        "best/accuracy_improvement": acc_improvement,
                        "best/f1_improvement": f1_improvement,
                        "best/learning_rate": current_lr
                    })
            else:
                patience_counter += 1
                no_improvement_counter += 1
                
                # ì„±ëŠ¥ ì €í•˜ ì •ë„ ì²´í¬ (ì¡°ê±´ë¶€ ì¶œë ¥)
                if acc_improvement < -min_delta and f1_improvement < -min_delta and should_print:
                    print(f"âš ï¸  ì„±ëŠ¥ ì €í•˜ ê°ì§€: Acc {acc_improvement:+.4f}, F1 {f1_improvement:+.4f}")
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            self.save_checkpoint(model, optimizer, epoch, val_metrics, is_best)
            
            # ê¸°ë¡ ì €ì¥
            train_history.append({
                'epoch': epoch + 1,
                'train': train_metrics,
                'val': val_metrics,
                'is_best': is_best,
                'improvements': {
                    'accuracy': acc_improvement,
                    'f1': f1_improvement,
                    'composite': composite_improvement
                }
            })
            
            # Early Stopping ì²´í¬ (í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ)
            if early_stopping_enabled:
                # ì—„ê²©í•œ Early Stopping ì¡°ê±´ë“¤
                early_stop_reasons = []
                
                # 1. ê¸°ë³¸ patience ì´ˆê³¼
                if patience_counter >= patience:
                    early_stop_reasons.append(f"patience {patience} ì´ˆê³¼")
                
                # 2. ì—°ì† ì„±ëŠ¥ ì €í•˜ê°€ ë„ˆë¬´ ë§ìŒ (patienceì˜ 1.5ë°°)
                if no_improvement_counter >= int(patience * 1.5):
                    early_stop_reasons.append(f"ì—°ì† {no_improvement_counter}íšŒ ì„±ëŠ¥ í–¥ìƒ ì—†ìŒ")
                
                # 3. ì„±ëŠ¥ì´ ì‹¬ê°í•˜ê²Œ ì €í•˜ë˜ê³  ìˆìŒ
                if (acc_improvement < -min_delta * 3 and f1_improvement < -min_delta * 3 and 
                    no_improvement_counter >= 3):
                    early_stop_reasons.append("ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ ê°ì§€")
                
                if early_stop_reasons:
                    print(f"â° Early stopping at epoch {epoch+1}")
                    print(f"   ì´ìœ : {', '.join(early_stop_reasons)}")
                    print(f"   ìµœê³  ì„±ëŠ¥: Acc {best_val_acc:.4f}, F1 {best_val_f1:.4f}")
                    
                    if self.use_wandb:
                        wandb.log({
                            "early_stopped": True, 
                            "stopped_epoch": epoch + 1,
                            "stop_reasons": early_stop_reasons,
                            "final_patience_counter": patience_counter,
                            "final_no_improvement_counter": no_improvement_counter,
                            "final_best_accuracy": best_val_acc,
                            "final_best_f1": best_val_f1,
                            "final_composite_score": best_composite_score,
                            "final_val_accuracy": val_metrics['accuracy'],
                            "final_val_f1": val_metrics['f1'],
                            "final_accuracy_drop": best_val_acc - val_metrics['accuracy'],
                            "final_f1_drop": best_val_f1 - val_metrics['f1']
                        })
                    break
        
        # ìµœê³  ëª¨ë¸ë¡œ validation ì¬í‰ê°€ (ìƒì„¸ ë¦¬í¬íŠ¸)
        print(f"\nğŸ¯ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ validation ìƒì„¸ í‰ê°€")
        best_model_path = os.path.join(self.checkpoint_dir, 'best_model.pth')
        val_detailed_metrics = self.evaluate_model_detailed(val_loader, best_model_path, "Validation")
        
        # ìµœê³  ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ¯ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ í‰ê°€")
        test_metrics = self.evaluate_model_detailed(test_loader, best_model_path, "Test")
        
        # wandbì— ìµœì¢… ê²°ê³¼ ë¡œê·¸
        if self.use_wandb:
            wandb.log({
                "final_validation/accuracy": val_detailed_metrics['accuracy'],
                "final_validation/precision": val_detailed_metrics['precision'],
                "final_validation/recall": val_detailed_metrics['recall'],
                "final_validation/f1": val_detailed_metrics['f1'],
                "final_validation/auc": val_detailed_metrics['auc'],
                "final_test/accuracy": test_metrics['accuracy'],
                "final_test/precision": test_metrics['precision'],
                "final_test/recall": test_metrics['recall'],
                "final_test/f1": test_metrics['f1'],
                "final_test/auc": test_metrics['auc']
            })
            
            # ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥
            if self.config["wandb"]["save_model"]:
                model_artifact = wandb.Artifact(
                    f"morigirl-model-{self.experiment_name}", 
                    type="model",
                    description=f"Best model from experiment {self.experiment_name}"
                )
                model_artifact.add_file(best_model_path)
                wandb.log_artifact(model_artifact)
        
        # í•™ìŠµ ê¸°ë¡ ì €ì¥
        history_path = os.path.join(self.result_dir, 'training_history.json')
        with open(history_path, 'w') as f:
            json.dump(train_history, f, indent=2)
        
        # wandb ì¢…ë£Œ
        if self.use_wandb:
            wandb.finish()
        
        print(f"âœ… í•™ìŠµ ì™„ë£Œ! ê²°ê³¼ í´ë”: {self.result_dir}")

    def evaluate_model_detailed(self, data_loader: DataLoader, model_path: str, dataset_name: str) -> Dict[str, float]:
        """ëª¨ë¸ ìƒì„¸ í‰ê°€"""
        # ëª¨ë¸ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # configì—ì„œ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì½ê¸°
        model_config = self.config["model"]
        model_kwargs = {
            "input_dim": model_config["input_vector_dim"],
            "hidden_dim": model_config["hidden_dim"],
            "hidden_dim2": model_config["hidden_dim2"],
            "dropout_rate": model_config["dropout_rate"]
        }
        
        model = MoriGirlVectorClassifier(**model_kwargs)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()
        
        all_preds = []
        all_labels = []
        all_probs = []
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc=f"{dataset_name} í‰ê°€"):
                vectors = batch['vector'].to(self.device)
                labels = batch['label'].to(self.device)
                
                outputs = model(vectors)
                probs = outputs.cpu().numpy().flatten()
                preds = (probs > 0.5).astype(int)
                
                all_probs.extend(probs)
                all_preds.extend(preds.astype(int))  # ëª…ì‹œì ìœ¼ë¡œ int íƒ€ì… ë³´ì¥
                all_labels.extend(labels.cpu().numpy())
        
        # ìµœì¢… ì„±ëŠ¥ ê³„ì‚°
        # ì—°ì†ê°’ ë¼ë²¨ì„ ì´ì§„ê°’ìœ¼ë¡œ ë³€í™˜ (ë©”íŠ¸ë¦­ ê³„ì‚°ìš©)
        binary_labels = (np.array(all_labels) >= 0.5).astype(int)
        binary_preds = np.array(all_preds).astype(int)  # ì˜ˆì¸¡ê°’ë„ ëª…ì‹œì ìœ¼ë¡œ ì´ì§„ê°’ìœ¼ë¡œ ë³€í™˜
        
        accuracy = accuracy_score(binary_labels, binary_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            binary_labels, binary_preds, average='binary', zero_division=0
        )
        auc = roc_auc_score(binary_labels, all_probs)
        
        print(f"\nğŸ“Š ìµœì¢… {dataset_name} ì„±ëŠ¥:")
        print(f"  - ì •í™•ë„: {accuracy:.4f}")
        print(f"  - ì •ë°€ë„: {precision:.4f}")
        print(f"  - ì¬í˜„ìœ¨: {recall:.4f}")
        print(f"  - F1 ì ìˆ˜: {f1:.4f}")
        print(f"  - AUC: {auc:.4f}")
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print(f"\nğŸ“‹ {dataset_name} ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        print(classification_report(binary_labels, binary_preds, target_names=['ë¹„ëª¨ë¦¬ê±¸', 'ëª¨ë¦¬ê±¸']))
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc
        }

def main():
    parser = argparse.ArgumentParser(description='ëª¨ë¦¬ê±¸ ë²¡í„° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--config-path', default='config.json', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--data-path', default=None, help='ë°ì´í„° ê²½ë¡œ (ì„¤ì • íŒŒì¼ ìš°ì„ )')
    parser.add_argument('--experiment-name', help='ì‹¤í—˜ ì´ë¦„')
    
    args = parser.parse_args()
    
    try:
        # í•™ìŠµ ì‹œì‘
        trainer = MoriGirlTrainer(
            config_path=args.config_path,
            data_path=args.data_path,
            experiment_name=args.experiment_name
        )
        
        # config.json ê¸°ë°˜ í•™ìŠµ
        trainer.train()
        
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 