#!/usr/bin/env python3
# prepare_training_data.py

import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from pathlib import Path
import json
from typing import List, Dict, Tuple, Any

class MorigirlDataset(Dataset):
    """ëª¨ë¦¬ê±¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ PyTorch Dataset"""
    
    def __init__(self, vectors: np.ndarray, labels: np.ndarray, product_ids: np.ndarray):
        """
        Args:
            vectors: ì´ë¯¸ì§€ íŠ¹ì§• ë²¡í„° (N, vector_dim)
            labels: ëª¨ë¦¬ê±¸ ì—¬ë¶€ ë¼ë²¨ (N,) - 1: ëª¨ë¦¬ê±¸, 0: ë¹„ëª¨ë¦¬ê±¸
            product_ids: ìƒí’ˆ ID (N,)
        """
        self.vectors = torch.FloatTensor(vectors)
        self.labels = torch.FloatTensor(labels)  # BCELossë¥¼ ìœ„í•´ Float íƒ€ì…ìœ¼ë¡œ ë³€ê²½
        self.product_ids = product_ids
        
    def __len__(self):
        return len(self.vectors)
    
    def __getitem__(self, idx):
        return {
            'vector': self.vectors[idx],
            'label': self.labels[idx],
            'product_id': self.product_ids[idx]
        }

class MorigirlDataProcessor:
    """ëª¨ë¦¬ê±¸ í•™ìŠµ ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def __init__(self, data_dir: str = "../data/morigirl_50"):
        self.data_dir = data_dir
        self.vectors = []
        self.labels = []
        self.product_ids = []
        
    def load_npy_files(self, split_type: str = "all") -> bool:
        """npy íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ í•™ìŠµìš© ë°ì´í„°ë¡œ ë³€í™˜
        
        Args:
            split_type: "all" (ì „ì²´), "train" (trainë§Œ), "test" (testë§Œ)
        """
        
        data_path = Path(self.data_dir)
        if not data_path.exists():
            print(f"âŒ ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {self.data_dir}")
            return False
        
        # split_typeì— ë”°ë¼ íŒŒì¼ í•„í„°ë§
        if split_type == "train":
            pattern = "*_train.npy"
        elif split_type == "test":
            pattern = "*_test.npy"
        else:  # "all"
            pattern = "*.npy"
            
        npy_files = list(data_path.glob(pattern))
        if not npy_files:
            print(f"âŒ {self.data_dir} í´ë”ì— {pattern} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"ğŸ“ ë¡œë”©í•  npy íŒŒì¼ ({split_type}): {len(npy_files)}ê°œ")
        
        total_loaded = 0
        morigirl_count = 0
        non_morigirl_count = 0
        
        for npy_file in sorted(npy_files):
            print(f"ğŸ”„ ë¡œë”© ì¤‘: {npy_file.name}")
            
            try:
                data = np.load(npy_file, allow_pickle=True)
                
                for record in data:
                    if isinstance(record, dict):
                        vector = record.get('vector')
                        is_morigirl = record.get('is_morigirl')
                        product_id = record.get('product_id')
                        
                        # í•„ìˆ˜ ë°ì´í„° í™•ì¸
                        if (vector is not None and 
                            is_morigirl is not None and 
                            product_id is not None):
                            
                            # ë²¡í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
                            if isinstance(vector, list):
                                vector = np.array(vector)
                            elif not isinstance(vector, np.ndarray):
                                continue
                            
                            # ë²¡í„° ì°¨ì› í™•ì¸ (ì¼ë°˜ì ìœ¼ë¡œ 1024ì°¨ì›)
                            if len(vector.shape) == 1 and len(vector) > 0:
                                self.vectors.append(vector)
                                self.labels.append(int(is_morigirl))
                                self.product_ids.append(product_id)
                                
                                if is_morigirl == 1:
                                    morigirl_count += 1
                                else:
                                    non_morigirl_count += 1
                                
                                total_loaded += 1
                
                print(f"  âœ… {npy_file.name}: {len(data)}ê°œ ë¡œë”©")
                
            except Exception as e:
                print(f"âŒ {npy_file.name} ë¡œë”© ì‹¤íŒ¨: {e}")
        
        if total_loaded == 0:
            print("âŒ ë¡œë”©ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        self.vectors = np.vstack(self.vectors)
        self.labels = np.array(self.labels)
        self.product_ids = np.array(self.product_ids)
        
        print(f"\nğŸ“Š ë°ì´í„° ë¡œë”© ì™„ë£Œ ({split_type}):")
        print(f"  - ì´ ë°ì´í„°: {total_loaded:,}ê°œ")
        print(f"  - ëª¨ë¦¬ê±¸: {morigirl_count:,}ê°œ ({morigirl_count/total_loaded*100:.1f}%)")
        print(f"  - ë¹„ëª¨ë¦¬ê±¸: {non_morigirl_count:,}ê°œ ({non_morigirl_count/total_loaded*100:.1f}%)")
        print(f"  - ë²¡í„° ì°¨ì›: {self.vectors.shape[1]}")
        
        return True
    
    def create_train_test_split(self, test_size: float = 0.2, random_state: int = 42) -> Tuple[MorigirlDataset, MorigirlDataset]:
        """train/test ë°ì´í„°ì…‹ ë¶„í• """
        
        if len(self.vectors) == 0:
            raise ValueError("ë°ì´í„°ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_npy_files()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ê³„ì¸µí™” ë¶„í•  (ëª¨ë¦¬ê±¸/ë¹„ëª¨ë¦¬ê±¸ ë¹„ìœ¨ ìœ ì§€)
        X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(
            self.vectors, self.labels, self.product_ids,
            test_size=test_size,
            random_state=random_state,
            stratify=self.labels  # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
        )
        
        # Dataset ê°ì²´ ìƒì„±
        train_dataset = MorigirlDataset(X_train, y_train, ids_train)
        test_dataset = MorigirlDataset(X_test, y_test, ids_test)
        
        print(f"\nğŸ”„ Train/Test ë¶„í•  ì™„ë£Œ:")
        print(f"  - Train: {len(train_dataset):,}ê°œ (ëª¨ë¦¬ê±¸: {np.sum(y_train):,}ê°œ)")
        print(f"  - Test: {len(test_dataset):,}ê°œ (ëª¨ë¦¬ê±¸: {np.sum(y_test):,}ê°œ)")
        print(f"  - Test ë¹„ìœ¨: {test_size*100:.1f}%")
        
        return train_dataset, test_dataset
    
    def create_dataloaders(self, train_dataset: MorigirlDataset, test_dataset: MorigirlDataset,
                          batch_size: int = 32, num_workers: int = 0) -> Tuple[DataLoader, DataLoader]:
        """DataLoader ìƒì„±"""
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        print(f"ğŸ“¦ DataLoader ìƒì„± ì™„ë£Œ:")
        print(f"  - Train batches: {len(train_loader)}")
        print(f"  - Test batches: {len(test_loader)}")
        print(f"  - Batch size: {batch_size}")
        
        return train_loader, test_loader
    
    def save_processed_data(self, train_dataset: MorigirlDataset, test_dataset: MorigirlDataset,
                           output_dir: str = "../data/processed"):
        """ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Train ë°ì´í„° ì €ì¥
        train_data = {
            'vectors': train_dataset.vectors.numpy(),
            'labels': train_dataset.labels.numpy(),
            'product_ids': train_dataset.product_ids
        }
        np.save(f"{output_dir}/train_data.npy", train_data)
        
        # Test ë°ì´í„° ì €ì¥
        test_data = {
            'vectors': test_dataset.vectors.numpy(),
            'labels': test_dataset.labels.numpy(),
            'product_ids': test_dataset.product_ids
        }
        np.save(f"{output_dir}/test_data.npy", test_data)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'vector_dim': train_dataset.vectors.shape[1],
            'num_classes': 2,
            'train_size': len(train_dataset),
            'test_size': len(test_dataset),
            'train_morigirl_count': int(torch.sum(train_dataset.labels)),
            'test_morigirl_count': int(torch.sum(test_dataset.labels)),
            'class_names': ['non_morigirl', 'morigirl']
        }
        
        with open(f"{output_dir}/metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥:")
        print(f"  - {output_dir}/train_data.npy")
        print(f"  - {output_dir}/test_data.npy")
        print(f"  - {output_dir}/metadata.json")
    
    def get_data_info(self) -> Dict[str, Any]:
        """ë°ì´í„° ì •ë³´ ë°˜í™˜"""
        if len(self.vectors) == 0:
            return {}
        
        return {
            'total_samples': len(self.vectors),
            'vector_dim': self.vectors.shape[1],
            'morigirl_count': np.sum(self.labels),
            'non_morigirl_count': len(self.labels) - np.sum(self.labels),
            'morigirl_ratio': np.mean(self.labels)
        }

 