# dataset/morigirl_vector_dataset.py

import os
import torch
from torch.utils.data import Dataset, DataLoader, ConcatDataset
import numpy as np
from typing import List, Tuple, Dict, Any, Optional
from sklearn.model_selection import train_test_split
import json

class MoriGirlVectorDataset(Dataset):
    """
    ì´ë¯¸ì§€ ë²¡í„° ê¸°ë°˜ ëª¨ë¦¬ê±¸ ë¶„ë¥˜ ë°ì´í„°ì…‹
    
    save_image_vectors.pyì—ì„œ ìƒì„±í•œ npy íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ ì‚¬ìš©
    """
    
    def __init__(self, 
                 data_path: str,
                 data_type: str = "all",  # "morigirl", "non_morigirl", "all"
                 normalize_vectors: bool = True,
                 normalize_prices: bool = True):
        """
        Args:
            data_path: data/training_data í´ë” ê²½ë¡œ
            data_type: ë¡œë“œí•  ë°ì´í„° íƒ€ì…
            normalize_vectors: ë²¡í„° ì •ê·œí™” ì—¬ë¶€
            normalize_prices: ê°€ê²© ì •ê·œí™” ì—¬ë¶€
        """
        self.data_path = data_path
        self.data_type = data_type
        self.normalize_vectors = normalize_vectors
        self.normalize_prices = normalize_prices
        
        # ë°ì´í„° ë¡œë“œ
        self.data = self._load_data()
        
        print(f"âœ… {data_type} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.data)}ê°œ")
        self._print_statistics()
    
    def _load_data(self) -> List[Dict[str, Any]]:
        """npy íŒŒì¼ë“¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        all_data = []
        
        if self.data_type in ["morigirl", "all"]:
            morigirl_data = self._load_npy_files("morigirl")
            all_data.extend(morigirl_data)
            print(f"ğŸ“¦ ëª¨ë¦¬ê±¸ ë°ì´í„°: {len(morigirl_data)}ê°œ")
        
        if self.data_type in ["non_morigirl", "all"]:
            non_morigirl_data = self._load_npy_files("non_morigirl")
            all_data.extend(non_morigirl_data)
            print(f"ğŸ“¦ ë¹„ëª¨ë¦¬ê±¸ ë°ì´í„°: {len(non_morigirl_data)}ê°œ")
        
        if not all_data:
            raise ValueError(f"ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_path}")
        
        # ë°ì´í„° í›„ì²˜ë¦¬
        all_data = self._postprocess_data(all_data)
        
        return all_data
    
    def _load_npy_files(self, file_prefix: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • prefixì˜ npy íŒŒì¼ë“¤ ë¡œë“œ"""
        data = []
        
        # í´ë”ì—ì„œ í•´ë‹¹ prefixë¡œ ì‹œì‘í•˜ëŠ” npy íŒŒì¼ë“¤ ì°¾ê¸°
        for filename in os.listdir(self.data_path):
            if filename.startswith(file_prefix) and filename.endswith('.npy'):
                file_path = os.path.join(self.data_path, filename)
                print(f"  ğŸ“ ë¡œë”©: {filename}")
                
                try:
                    # npy íŒŒì¼ ë¡œë“œ
                    file_data = np.load(file_path, allow_pickle=True)
                    
                    # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€
                    if isinstance(file_data, np.ndarray) and file_data.dtype == object:
                        # object ë°°ì—´ì¸ ê²½ìš° (ë”•ì…”ë„ˆë¦¬ë“¤ì˜ ë°°ì—´)
                        data.extend(file_data.tolist())
                    else:
                        # ì¼ë°˜ ë°°ì—´ì¸ ê²½ìš°
                        data.extend(file_data)
                    
                    print(f"    âœ… {len(file_data)}ê°œ ì¶”ê°€")
                    
                except Exception as e:
                    print(f"    âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        
        return data
    
    def _postprocess_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°ì´í„° í›„ì²˜ë¦¬ (ì •ê·œí™” ë“±)"""
        processed_data = []
        
        # ê°€ê²© ì •ê·œí™”ë¥¼ ìœ„í•œ í†µê³„
        if self.normalize_prices:
            prices = [item['price'] for item in data if 'price' in item]
            if prices:
                price_mean = np.mean(prices)
                price_std = np.std(prices)
                print(f"ğŸ“Š ê°€ê²© í†µê³„: í‰ê· ={price_mean:.2f}, í‘œì¤€í¸ì°¨={price_std:.2f}")
        
        for item in data:
            try:
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                if not all(key in item for key in ['vector', 'is_morigirl']):
                    continue
                
                # ë²¡í„° ì²˜ë¦¬
                vector = np.array(item['vector'], dtype=np.float32)
                if len(vector) != 1024:
                    print(f"âš ï¸ ë²¡í„° ì°¨ì› ì˜¤ë¥˜: {len(vector)}, ê±´ë„ˆëœ€")
                    continue
                
                # ë²¡í„° ì •ê·œí™”
                if self.normalize_vectors:
                    norm = np.linalg.norm(vector)
                    if norm > 0:
                        vector = vector / norm
                
                # ê°€ê²© ì •ê·œí™”
                price = item.get('price', 0)
                if self.normalize_prices and 'price' in item and price_std > 0:
                    price = (price - price_mean) / price_std
                
                processed_item = {
                    'product_id': item.get('product_id', 0),
                    'vector': vector,
                    'price': price,
                    'is_morigirl': float(item['is_morigirl']),
                    'first_category': item.get('first_category', 0),
                    'second_category': item.get('second_category', 0),
                    'sales_score': item.get('sales_score', 0.0)
                }
                
                processed_data.append(processed_item)
                
            except Exception as e:
                print(f"âš ï¸ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        return processed_data
    
    def _print_statistics(self):
        """ë°ì´í„° í†µê³„ ì¶œë ¥"""
        if not self.data:
            return
        
        # í´ë˜ìŠ¤ ë¶„í¬
        morigirl_count = sum(1 for item in self.data if item['is_morigirl'] == 1.0)
        non_morigirl_count = len(self.data) - morigirl_count
        
        print(f"ğŸ“Š ë°ì´í„° í†µê³„:")
        print(f"  - ëª¨ë¦¬ê±¸: {morigirl_count}ê°œ ({morigirl_count/len(self.data)*100:.1f}%)")
        print(f"  - ë¹„ëª¨ë¦¬ê±¸: {non_morigirl_count}ê°œ ({non_morigirl_count/len(self.data)*100:.1f}%)")
        
        # ê°€ê²© í†µê³„
        prices = [item['price'] for item in self.data]
        if prices:
            print(f"  - ê°€ê²© ë²”ìœ„: {min(prices):.2f} ~ {max(prices):.2f}")
    
    def get_class_weights(self) -> float:
        """í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        morigirl_count = sum(1 for item in self.data if item['is_morigirl'] == 1.0)
        non_morigirl_count = len(self.data) - morigirl_count
        
        if morigirl_count > 0:
            pos_weight = non_morigirl_count / morigirl_count
            return pos_weight
        return 1.0
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        """
        ë°ì´í„° ìƒ˜í”Œ ë°˜í™˜
        
        Returns:
            vector: (1024,) ì´ë¯¸ì§€ ë²¡í„°
            label: ëª¨ë¦¬ê±¸ ë¼ë²¨ (0 ë˜ëŠ” 1)
        """
        item = self.data[idx]
        
        # ë²¡í„°ì™€ ë¼ë²¨ ë°˜í™˜
        vector = torch.tensor(item['vector'], dtype=torch.float32)
        label = torch.tensor(item['is_morigirl'], dtype=torch.float32)
        
        return vector, label
    
    def get_item_info(self, idx: int) -> Dict[str, Any]:
        """íŠ¹ì • ì¸ë±ìŠ¤ì˜ ìƒí’ˆ ì •ë³´ ë°˜í™˜"""
        return self.data[idx]

def create_train_test_datasets(
    data_path: str = "data/training_data",
    test_size: float = 0.2,
    val_size: float = 0.1,
    random_state: int = 42,
    **kwargs
) -> Tuple[Dataset, Dataset, Dataset]:
    """
    í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±
    
    Args:
        data_path: ë°ì´í„° í´ë” ê²½ë¡œ
        test_size: í…ŒìŠ¤íŠ¸ ì…‹ ë¹„ìœ¨
        val_size: ê²€ì¦ ì…‹ ë¹„ìœ¨ (í›ˆë ¨ ì…‹ ê¸°ì¤€)
        random_state: ëœë¤ ì‹œë“œ
        **kwargs: ë°ì´í„°ì…‹ ìƒì„±ì‹œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
    
    Returns:
        (train_dataset, val_dataset, test_dataset)
    """
    # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
    full_dataset = MoriGirlVectorDataset(data_path, data_type="all", **kwargs)
    
    # ì¸ë±ìŠ¤ ë¶„í• 
    indices = list(range(len(full_dataset)))
    labels = [full_dataset.data[i]['is_morigirl'] for i in indices]
    
    # train+val / test ë¶„í• 
    train_val_indices, test_indices = train_test_split(
        indices, test_size=test_size, random_state=random_state, 
        stratify=labels
    )
    
    # train / val ë¶„í• 
    train_val_labels = [labels[i] for i in train_val_indices]
    train_indices, val_indices = train_test_split(
        train_val_indices, test_size=val_size, random_state=random_state,
        stratify=train_val_labels
    )
    
    # ì„œë¸Œì…‹ ìƒì„±
    train_data = [full_dataset.data[i] for i in train_indices]
    val_data = [full_dataset.data[i] for i in val_indices]
    test_data = [full_dataset.data[i] for i in test_indices]
    
    # ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±
    train_dataset = MoriGirlVectorDataset.__new__(MoriGirlVectorDataset)
    train_dataset.__dict__.update(full_dataset.__dict__)
    train_dataset.data = train_data
    
    val_dataset = MoriGirlVectorDataset.__new__(MoriGirlVectorDataset)
    val_dataset.__dict__.update(full_dataset.__dict__)
    val_dataset.data = val_data
    
    test_dataset = MoriGirlVectorDataset.__new__(MoriGirlVectorDataset)
    test_dataset.__dict__.update(full_dataset.__dict__)
    test_dataset.data = test_data
    
    print(f"ğŸ“Š ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:")
    print(f"  - í›ˆë ¨: {len(train_dataset)}ê°œ")
    print(f"  - ê²€ì¦: {len(val_dataset)}ê°œ")
    print(f"  - í…ŒìŠ¤íŠ¸: {len(test_dataset)}ê°œ")
    
    return train_dataset, val_dataset, test_dataset

def create_dataloader(
    dataset: Dataset,
    batch_size: int = 32,
    shuffle: bool = True,
    num_workers: int = 4,
    **kwargs
) -> DataLoader:
    """DataLoader ìƒì„±"""
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
        **kwargs
    )

# í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜
if __name__ == "__main__":
    # ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
    try:
        dataset = MoriGirlVectorDataset("data/training_data")
        print(f"ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
        
        # ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸
        vector, label = dataset[0]
        print(f"ë²¡í„° í˜•íƒœ: {vector.shape}")
        print(f"ë¼ë²¨: {label}")
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜
        pos_weight = dataset.get_class_weights()
        print(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight:.2f}")
        
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}") 