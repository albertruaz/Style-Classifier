#!/usr/bin/env python3
# test_trained_model.py

import os
import json
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    precision_recall_curve, roc_curve, accuracy_score,
    precision_recall_fscore_support
)
from torch.utils.data import DataLoader
from tqdm import tqdm
import argparse
from typing import Dict, List, Tuple, Any
from datetime import datetime

# ë¡œì»¬ ëª¨ë“ˆ
from prepare_training_data import MorigirlDataProcessor, MorigirlDataset
from model.morigirl_model import MoriGirlVectorClassifier

class MoriGirlModelTester:
    """í•™ìŠµëœ ëª¨ë¦¬ê±¸ ë²¡í„° ë¶„ë¥˜ ëª¨ë¸ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, checkpoint_path: str, data_path: str = "data/morigirl_50"):
        self.checkpoint_path = checkpoint_path
        self.data_path = data_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ì›”ì¼ì‹œë¶„_ëœë¤2ìë¦¬)
        import random
        date_str = datetime.now().strftime('%m%d%H%M')  # ì›”ì¼ì‹œë¶„
        random_num = random.randint(10, 99)  # ëœë¤ 2ìë¦¬
        result_name = f"{date_str}_{random_num:02d}"
        self.results_dir = f"result/{result_name}"
        os.makedirs(self.results_dir, exist_ok=True)
        
        print(f"ğŸ§ª ëª¨ë¦¬ê±¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"  - ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}")
        print(f"  - ë°ì´í„° ê²½ë¡œ: {data_path}")
        print(f"  - ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"  - ê²°ê³¼ ì €ì¥: {self.results_dir}")

    def load_model(self) -> nn.Module:
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        print(f"\nğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
        
        # ëª¨ë¸ ìƒì„±
        model = MoriGirlVectorClassifier()
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"  - ì—í¬í¬: {checkpoint.get('epoch', 'N/A')}")
        print(f"  - ê²€ì¦ ì •í™•ë„: {checkpoint.get('metrics', {}).get('accuracy', 'N/A')}")
        
        return model

    def setup_test_dataset(self, batch_size: int = 32) -> DataLoader:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„¤ì •"""
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„¤ì •")
        
        # ë°ì´í„° ì²˜ë¦¬ê¸° ìƒì„± ë° ë¡œë”©
        processor = MorigirlDataProcessor(self.data_path)
        if not processor.load_npy_files():
            raise RuntimeError("ë°ì´í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # Train/Test ë¶„í•  (í…ŒìŠ¤íŠ¸ ì…‹ë§Œ ì‚¬ìš©)
        _, test_dataset = processor.create_train_test_split(
            test_size=0.2, random_state=42
        )
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        _, test_loader = processor.create_dataloaders(test_dataset, test_dataset, batch_size)
        
        return test_loader

    def predict_all(self, model: nn.Module, test_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì…‹ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰"""
        print(f"\nğŸ”® ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰")
        
        all_probs = []
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="ì˜ˆì¸¡ ì¤‘"):
                vectors = batch['vector'].to(self.device)
                labels = batch['label']
                
                # ì˜ˆì¸¡ ìˆ˜í–‰
                outputs = model(vectors)  # ì´ë¯¸ sigmoid ì ìš©ë¨
                probs = outputs.cpu().numpy().flatten()
                preds = (probs > 0.5).astype(int)
                
                all_probs.extend(probs)
                all_preds.extend(preds)
                all_labels.extend(labels.numpy())
        
        return np.array(all_probs), np.array(all_preds), np.array(all_labels)

    def compute_metrics(self, probs: np.ndarray, preds: np.ndarray, labels: np.ndarray) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print(f"\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°")
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        accuracy = accuracy_score(labels, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels, preds, average='binary', zero_division=0
        )
        
        # AUC
        try:
            auc = roc_auc_score(labels, probs)
        except:
            auc = 0.0
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(labels, preds)
        
        # ROC ê³¡ì„  ë°ì´í„°
        fpr, tpr, _ = roc_curve(labels, probs)
        
        # Precision-Recall ê³¡ì„  ë°ì´í„°
        pr_precision, pr_recall, _ = precision_recall_curve(labels, probs)
        
        # í´ë˜ìŠ¤ë³„ ë¶„ë¥˜ ë¦¬í¬íŠ¸
        report = classification_report(labels, preds, target_names=['ë¹„ëª¨ë¦¬ê±¸', 'ëª¨ë¦¬ê±¸'], output_dict=True)
        
        metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'confusion_matrix': cm,
            'roc_curve': (fpr, tpr),
            'pr_curve': (pr_precision, pr_recall),
            'classification_report': report
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"âœ… ì„±ëŠ¥ ê²°ê³¼:")
        print(f"  - ì •í™•ë„: {accuracy:.4f}")
        print(f"  - ì •ë°€ë„: {precision:.4f}")
        print(f"  - ì¬í˜„ìœ¨: {recall:.4f}")
        print(f"  - F1 ì ìˆ˜: {f1:.4f}")
        print(f"  - AUC: {auc:.4f}")
        
        return metrics

    def create_visualizations(self, metrics: Dict[str, Any], probs: np.ndarray, labels: np.ndarray):
        """ì‹œê°í™” ìƒì„±"""
        print(f"\nğŸ“ˆ ì‹œê°í™” ìƒì„±")
        
        # í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œìŠ¤í…œì— ë”°ë¼ ì¡°ì • í•„ìš”)
        plt.rcParams['font.family'] = ['AppleGothic', 'Malgun Gothic', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('ëª¨ë¦¬ê±¸ ë¶„ë¥˜ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼', fontsize=16)
        
        # 1. í˜¼ë™ í–‰ë ¬
        ax1 = axes[0, 0]
        cm = metrics['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
                   xticklabels=['ë¹„ëª¨ë¦¬ê±¸', 'ëª¨ë¦¬ê±¸'], yticklabels=['ë¹„ëª¨ë¦¬ê±¸', 'ëª¨ë¦¬ê±¸'])
        ax1.set_title('í˜¼ë™ í–‰ë ¬')
        ax1.set_xlabel('ì˜ˆì¸¡')
        ax1.set_ylabel('ì‹¤ì œ')
        
        # 2. ROC ê³¡ì„ 
        ax2 = axes[0, 1]
        fpr, tpr = metrics['roc_curve']
        ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {metrics["auc"]:.3f})')
        ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        ax2.set_xlim([0.0, 1.0])
        ax2.set_ylim([0.0, 1.05])
        ax2.set_xlabel('False Positive Rate')
        ax2.set_ylabel('True Positive Rate')
        ax2.set_title('ROC ê³¡ì„ ')
        ax2.legend(loc="lower right")
        ax2.grid(True)
        
        # 3. Precision-Recall ê³¡ì„ 
        ax3 = axes[0, 2]
        precision, recall = metrics['pr_curve']
        ax3.plot(recall, precision, color='blue', lw=2)
        ax3.set_xlabel('Recall')
        ax3.set_ylabel('Precision')
        ax3.set_title('Precision-Recall ê³¡ì„ ')
        ax3.grid(True)
        
        # 4. í™•ë¥  ë¶„í¬
        ax4 = axes[1, 0]
        morigirl_probs = probs[labels == 1]
        non_morigirl_probs = probs[labels == 0]
        
        ax4.hist(non_morigirl_probs, bins=30, alpha=0.7, label='ë¹„ëª¨ë¦¬ê±¸', color='red', density=True)
        ax4.hist(morigirl_probs, bins=30, alpha=0.7, label='ëª¨ë¦¬ê±¸', color='blue', density=True)
        ax4.axvline(x=0.5, color='black', linestyle='--', label='ì„ê³„ê°’ (0.5)')
        ax4.set_xlabel('ì˜ˆì¸¡ í™•ë¥ ')
        ax4.set_ylabel('ë°€ë„')
        ax4.set_title('í´ë˜ìŠ¤ë³„ í™•ë¥  ë¶„í¬')
        ax4.legend()
        ax4.grid(True)
        
        # 5. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°” ì°¨íŠ¸
        ax5 = axes[1, 1]
        metric_names = ['ì •í™•ë„', 'ì •ë°€ë„', 'ì¬í˜„ìœ¨', 'F1 ì ìˆ˜', 'AUC']
        metric_values = [metrics['accuracy'], metrics['precision'], 
                        metrics['recall'], metrics['f1'], metrics['auc']]
        
        bars = ax5.bar(metric_names, metric_values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])
        ax5.set_ylim(0, 1.0)
        ax5.set_title('ì„±ëŠ¥ ë©”íŠ¸ë¦­')
        ax5.set_ylabel('ì ìˆ˜')
        
        # ë°” ìœ„ì— ìˆ˜ì¹˜ í‘œì‹œ
        for bar, value in zip(bars, metric_values):
            ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{value:.3f}', ha='center', va='bottom')
        
        # 6. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
        ax6 = axes[1, 2]
        report = metrics['classification_report']
        
        class_names = ['ë¹„ëª¨ë¦¬ê±¸', 'ëª¨ë¦¬ê±¸']
        precision_scores = [report['0']['precision'], report['1']['precision']]
        recall_scores = [report['0']['recall'], report['1']['recall']]
        f1_scores = [report['0']['f1-score'], report['1']['f1-score']]
        
        x = np.arange(len(class_names))
        width = 0.25
        
        ax6.bar(x - width, precision_scores, width, label='ì •ë°€ë„', alpha=0.8)
        ax6.bar(x, recall_scores, width, label='ì¬í˜„ìœ¨', alpha=0.8)
        ax6.bar(x + width, f1_scores, width, label='F1 ì ìˆ˜', alpha=0.8)
        
        ax6.set_xlabel('í´ë˜ìŠ¤')
        ax6.set_ylabel('ì ìˆ˜')
        ax6.set_title('í´ë˜ìŠ¤ë³„ ì„±ëŠ¥')
        ax6.set_xticks(x)
        ax6.set_xticklabels(class_names)
        ax6.legend()
        ax6.grid(True)
        
        plt.tight_layout()
        
        # ì €ì¥
        save_path = os.path.join(self.results_dir, 'test_results_visualization.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ ì‹œê°í™” ì €ì¥: {save_path}")
        
        # ë³´ì—¬ì£¼ê¸° (ì„ íƒì‚¬í•­)
        # plt.show()
        plt.close()

    def save_detailed_results(self, metrics: Dict[str, Any], probs: np.ndarray, 
                            preds: np.ndarray, labels: np.ndarray):
        """ìƒì„¸ ê²°ê³¼ ì €ì¥"""
        print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥")
        
        # 1. ë©”íŠ¸ë¦­ JSON ì €ì¥
        metrics_to_save = {
            'accuracy': float(metrics['accuracy']),
            'precision': float(metrics['precision']),
            'recall': float(metrics['recall']),
            'f1': float(metrics['f1']),
            'auc': float(metrics['auc']),
            'confusion_matrix': metrics['confusion_matrix'].tolist(),
            'classification_report': metrics['classification_report']
        }
        
        metrics_path = os.path.join(self.results_dir, 'metrics.json')
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics_to_save, f, ensure_ascii=False, indent=2)
        
        # 2. ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥
        results_df = pd.DataFrame({
            'actual_label': labels,
            'predicted_label': preds,
            'predicted_probability': probs,
            'correct': labels == preds
        })
        
        csv_path = os.path.join(self.results_dir, 'predictions.csv')
        results_df.to_csv(csv_path, index=False)
        
        # 3. ë¶„ë¥˜ ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ ì €ì¥
        report_text = classification_report(labels, preds, target_names=['ë¹„ëª¨ë¦¬ê±¸', 'ëª¨ë¦¬ê±¸'])
        report_path = os.path.join(self.results_dir, 'classification_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"  - ë©”íŠ¸ë¦­: {metrics_path}")
        print(f"  - ì˜ˆì¸¡ ê²°ê³¼: {csv_path}")
        print(f"  - ë¶„ë¥˜ ë¦¬í¬íŠ¸: {report_path}")

    def run_comprehensive_test(self, batch_size: int = 32):
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ìˆ˜í–‰"""
        print(f"ğŸš€ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # 1. ëª¨ë¸ ë¡œë“œ
        model = self.load_model()
        
        # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„¤ì •
        test_loader = self.setup_test_dataset(batch_size)
        
        # 3. ì˜ˆì¸¡ ìˆ˜í–‰
        probs, preds, labels = self.predict_all(model, test_loader)
        
        # 4. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self.compute_metrics(probs, preds, labels)
        
        # 5. ì‹œê°í™” ìƒì„±
        self.create_visualizations(metrics, probs, labels)
        
        # 6. ìƒì„¸ ê²°ê³¼ ì €ì¥
        self.save_detailed_results(metrics, probs, preds, labels)
        
        print(f"\nâœ… ì¢…í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.results_dir}")
        
        return metrics

    def quick_test(self, num_samples: int = 10):
        """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ëª‡ ê°œ ìƒ˜í”Œë§Œ)"""
        print(f"\nâš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ({num_samples}ê°œ ìƒ˜í”Œ)")
        
        # ëª¨ë¸ ë¡œë“œ
        model = self.load_model()
        
        # ë°ì´í„° ì²˜ë¦¬ê¸°ë¡œ ë°ì´í„° ë¡œë“œ
        processor = MorigirlDataProcessor(self.data_path)
        if not processor.load_npy_files():
            raise RuntimeError("ë°ì´í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        _, test_dataset = processor.create_train_test_split(test_size=0.2, random_state=42)
        
        # ëœë¤ ìƒ˜í”Œ ì„ íƒ
        indices = np.random.choice(len(test_dataset), min(num_samples, len(test_dataset)), replace=False)
        
        print(f"\nğŸ”® ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼:")
        print(f"{'ì¸ë±ìŠ¤':<8} {'ì‹¤ì œ':<8} {'ì˜ˆì¸¡':<8} {'í™•ë¥ ':<10} {'ìƒí’ˆID':<12} {'ì •ë‹µ'}")
        print("-" * 70)
        
        correct = 0
        for i, idx in enumerate(indices):
            sample = test_dataset[idx]
            vector = sample['vector'].unsqueeze(0).to(self.device)
            label = sample['label']
            product_id = sample['product_id']
            
            with torch.no_grad():
                prob = model(vector).item()
                pred = int(prob > 0.5)
            
            is_correct = pred == int(label.item())
            correct += is_correct
            
            print(f"{idx:<8} {int(label.item()):<8} {pred:<8} {prob:<10.4f} {product_id:<12} {'âœ“' if is_correct else 'âœ—'}")
        
        accuracy = correct / len(indices)
        print(f"\nğŸ“Š ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.4f} ({correct}/{len(indices)})")

def main():
    parser = argparse.ArgumentParser(description='ëª¨ë¦¬ê±¸ ë²¡í„° ë¶„ë¥˜ ëª¨ë¸ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--checkpoint', required=True, help='ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
    parser.add_argument('--data-path', default='data/morigirl_50', help='í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ (ì˜ˆ: data/morigirl_50)')
    parser.add_argument('--batch-size', type=int, default=32, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--quick-test', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰')
    parser.add_argument('--num-samples', type=int, default=10, help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜')
    
    args = parser.parse_args()
    
    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(args.checkpoint):
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.checkpoint}")
        return
    
    # í…ŒìŠ¤í„° ìƒì„±
    tester = MoriGirlModelTester(args.checkpoint, args.data_path)
    
    if args.quick_test:
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        tester.quick_test(args.num_samples)
    else:
        # ì¢…í•© í…ŒìŠ¤íŠ¸
        tester.run_comprehensive_test(args.batch_size)

if __name__ == "__main__":
    main() 