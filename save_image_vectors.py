#!/usr/bin/env python3
# save_image_vectors.py

import os
import sys
import json
import requests
import numpy as np
from PIL import Image
import torch
import torchvision.transforms as transforms
from torchvision.models import efficientnet_b0
from tqdm import tqdm
import io
import hashlib
from uuid import UUID
from typing import List, Dict, Any, Tuple
from database import DatabaseManager

class ImageVectorExtractor:
    """ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ DBì— ì €ì¥"""
    
    def __init__(self, config_path: str = "./config.json"):
        with open(config_path, 'r') as f:
            self.config = json.load(f)
        
        # ë°ì´í„° ì €ì¥ í´ë”ëŠ” ë‚˜ì¤‘ì— max_productsì— ë”°ë¼ ì„¤ì •
        self.output_dir = None
        
        self.db_manager = DatabaseManager()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # EfficientNet ëª¨ë¸ ë¡œë“œ (íŠ¹ì§• ì¶”ì¶œìš©)
        self.model = efficientnet_b0(pretrained=True)
        self.model.classifier = torch.nn.Identity()  # ë¶„ë¥˜ì¸µ ì œê±°, íŠ¹ì§•ë§Œ ì¶”ì¶œ
        self.model.eval()
        self.model.to(self.device)
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        print(f"âœ… ì´ë¯¸ì§€ ë²¡í„° ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  - ë””ë°”ì´ìŠ¤: {self.device}")

    def uuid_to_bigint(self, uuid_val) -> int:
        """UUIDë¥¼ BIGINTë¡œ ë³€í™˜"""
        if isinstance(uuid_val, str):
            uuid_val = UUID(uuid_val)
        elif isinstance(uuid_val, UUID):
            pass
        else:
            # ì´ë¯¸ intì¸ ê²½ìš°
            return int(uuid_val)
        
        # UUIDë¥¼ bytesë¡œ ë³€í™˜ í›„ í•´ì‹œí•˜ì—¬ 64bit ì •ìˆ˜ë¡œ ë³€í™˜
        uuid_bytes = uuid_val.bytes
        hash_bytes = hashlib.sha256(uuid_bytes).digest()
        # ì²« 8ë°”ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ 64bit ì •ìˆ˜ ìƒì„±
        return int.from_bytes(hash_bytes[:8], byteorder='big', signed=True)

    def download_image(self, url: str, timeout: int = 10) -> Image.Image:
        """URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        try:
            response = requests.get(url, timeout=timeout)
            response.raise_for_status()
            
            image = Image.open(io.BytesIO(response.content))
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            return image
        except Exception as e:
            raise ValueError(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

    def extract_features(self, image: Image.Image) -> np.ndarray:
        """ì´ë¯¸ì§€ì—ì„œ 1024ì°¨ì› íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
        try:
            # ì „ì²˜ë¦¬
            input_tensor = self.transform(image).unsqueeze(0).to(self.device)
            
            # íŠ¹ì§• ì¶”ì¶œ
            with torch.no_grad():
                features = self.model(input_tensor)
                features = features.cpu().numpy().flatten()
            
            # L2 ì •ê·œí™”
            features = features / np.linalg.norm(features)
            
            return features
        except Exception as e:
            raise ValueError(f"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    def get_morigirl_products(self, limit: int = 1000) -> List[Dict]:
        """ëª¨ë¦¬ê±¸ ìŠ¤íƒ€ì¼ ìƒí’ˆ ì¡°íšŒ (styles_id = 9)"""
        mysql_session = self.db_manager.mysql.Session()
        try:
            from sqlalchemy import text
            
            sql = text("""
                SELECT DISTINCT 
                    p.id,
                    p.status,
                    p.amount,
                    p.views,
                    p.impressions,
                    p.primary_category_id,
                    p.secondary_category_id
                FROM vingle.product p
                JOIN vingle.product_styles ps ON p.id = ps.product_id
                WHERE ps.styles_id = '9'
                  AND p.status IN ('SALE', 'SOLD_OUT')
                  AND p.amount IS NOT NULL
                  AND p.views IS NOT NULL
                  AND p.impressions IS NOT NULL
                  AND p.impressions > 0
                ORDER BY RAND()
                LIMIT :limit
            """)
            
            result = mysql_session.execute(sql, {"limit": limit})
            
            products = []
            for row in result.fetchall():
                products.append({
                    'product_id': row[0],
                    'status': row[1],
                    'amount': row[2],
                    'views': row[3],
                    'impressions': row[4],
                    'primary_category_id': row[5],
                    'secondary_category_id': row[6],
                    'is_morigirl': 1  # ëª¨ë¦¬ê±¸
                })
            
            print(f"ğŸ¨ ëª¨ë¦¬ê±¸ ìŠ¤íƒ€ì¼ ìƒí’ˆ {len(products)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return products
            
        finally:
            mysql_session.close()

    def get_non_morigirl_products(self, limit: int = 1000) -> List[Dict]:
        """ë¹„ëª¨ë¦¬ê±¸ ìƒí’ˆ ì¡°íšŒ (styles_id != 9)"""
        mysql_session = self.db_manager.mysql.Session()
        try:
            from sqlalchemy import text
            
            sql = text("""
                SELECT DISTINCT 
                    p.id,
                    p.status,
                    p.amount,
                    p.views,
                    p.impressions,
                    p.primary_category_id,
                    p.secondary_category_id
                FROM vingle.product p
                WHERE p.id NOT IN (
                    SELECT DISTINCT product_id 
                    FROM vingle.product_styles 
                    WHERE styles_id = '9'
                )
                  AND p.status IN ('SALE', 'SOLD_OUT')
                  AND p.amount IS NOT NULL
                  AND p.views IS NOT NULL
                  AND p.impressions IS NOT NULL
                  AND p.impressions > 0
                ORDER BY RAND()
                LIMIT :limit
            """)
            
            result = mysql_session.execute(sql, {"limit": limit})
            
            products = []
            for row in result.fetchall():
                products.append({
                    'product_id': row[0],
                    'status': row[1],
                    'amount': row[2],
                    'views': row[3],
                    'impressions': row[4],
                    'primary_category_id': row[5],
                    'secondary_category_id': row[6],
                    'is_morigirl': 0  # ë¹„ëª¨ë¦¬ê±¸
                })
            
            print(f"ğŸ“¦ ë¹„ëª¨ë¦¬ê±¸ ìƒí’ˆ {len(products)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return products
            
        finally:
            mysql_session.close()

    def get_product_vectors(self, product_ids: List[int]) -> Dict[int, List[float]]:
        """Vector DBì—ì„œ ìƒí’ˆ ë²¡í„° ì¡°íšŒ"""
        if not product_ids:
            return {}
            
        vector_session = self.db_manager.vector_db.Session()
        try:
            from sqlalchemy import text
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            batch_size = 1000
            all_vectors = {}
            
            for i in range(0, len(product_ids), batch_size):
                batch_ids = product_ids[i:i + batch_size]
                
                # product_idë¡œ ì§ì ‘ ë¹„êµ
                placeholders = ','.join([str(batch_id) for batch_id in batch_ids])
                sql = text(f"""
                    SELECT product_id, vector
                    FROM product_image_vector
                    WHERE product_id IN ({placeholders})
                      AND vector IS NOT NULL
                """)
                
                result = vector_session.execute(sql)
                
                for product_id, vector_str in result.fetchall():
                    if vector_str:
                        # "[1.0,2.0,3.0,...]" í˜•ì‹ì„ íŒŒì‹±
                        if isinstance(vector_str, str):
                            vector_str = vector_str.strip('[]')
                            vector = [float(x) for x in vector_str.split(',')]
                        else:
                            # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ë‚˜ ë°°ì—´ì¸ ê²½ìš°
                            vector = list(vector_str)
                        all_vectors[int(product_id)] = vector
            
            return all_vectors
            
        finally:
            vector_session.close()

    def calculate_sales_score(self, status: str, views: int, impressions: int) -> float:
        """íŒë§¤ ì ìˆ˜ ê³„ì‚° (0~1 ì‚¬ì´ ê°’)"""
        if status == 'SOLD_OUT':
            return 1.0
        
        # views / impressions ë¹„ìœ¨ë¡œ ê³„ì‚°
        if impressions > 0:
            ratio = min(views / impressions, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
            return ratio
        else:
            return 0.0

    def save_training_data(self, products_data: List[Dict[str, Any]], data_type: str, 
                          total_processed: int):
        """í•™ìŠµìš© ë°ì´í„°ë¥¼ data í´ë”ì— ì €ì¥"""
        if not products_data:
            return False
        
        # ì„¸ì…˜ë³„ í´ë” ìƒì„±
        os.makedirs(self.output_dir, exist_ok=True)
        
        try:
            # ë°ì´í„° ì •ë¦¬
            training_data = []
            for item in products_data:
                training_item = {
                    'product_id': item['product_id'],
                    'price': item['amount'],
                    'vector': item['vector'],
                    'first_category': item['primary_category_id'],
                    'second_category': item['secondary_category_id'],
                    'is_morigirl': item['is_morigirl'],
                    'sales_score': item['sales_score']
                }
                training_data.append(training_item)
            
            # íŒŒì¼ ì €ì¥ (products ë‹¨ì–´ ì œê±°)
            filename = f"{self.output_dir}/{data_type}_{total_processed}.npy"
            np.save(filename, training_data)
            
            print(f"ğŸ“ {data_type} ë°ì´í„° ì €ì¥: {filename} ({len(training_data)}ê°œ)")
            return True
            
        except Exception as e:
            print(f"âŒ {data_type} ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def process_training_data(self, max_products_per_type: int = 5000):
        """í•™ìŠµìš© ë°ì´í„° ì²˜ë¦¬ - ëª¨ë¦¬ê±¸/ë¹„ëª¨ë¦¬ê±¸ ë¶„ë¦¬"""
        
        # ë°ì´í„° í´ë” ì„¤ì • (ìˆ«ìë§Œ ì‚¬ìš©)
        self.output_dir = f"data/morigirl_{max_products_per_type}"
        
        print(f"ğŸš€ í•™ìŠµìš© ë°ì´í„° ìƒì„± ì‹œì‘")
        print(f"  - ëª¨ë¦¬ê±¸ ìµœëŒ€: {max_products_per_type:,}ê°œ")
        print(f"  - ë¹„ëª¨ë¦¬ê±¸ ìµœëŒ€: {max_products_per_type:,}ê°œ")
        print(f"  - ì €ì¥ í´ë”: {self.output_dir}")
        
        # 1. ëª¨ë¦¬ê±¸ ë°ì´í„° ì²˜ë¦¬
        print(f"\n=== ëª¨ë¦¬ê±¸ ë°ì´í„° ì²˜ë¦¬ ===")
        morigirl_products = self.get_morigirl_products(max_products_per_type)
        morigirl_processed = self._process_product_batch(morigirl_products, "morigirl")
        
        # 2. ë¹„ëª¨ë¦¬ê±¸ ë°ì´í„° ì²˜ë¦¬
        print(f"\n=== ë¹„ëª¨ë¦¬ê±¸ ë°ì´í„° ì²˜ë¦¬ ===")
        non_morigirl_products = self.get_non_morigirl_products(max_products_per_type)
        non_morigirl_processed = self._process_product_batch(non_morigirl_products, "non_morigirl")
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ‰ í•™ìŠµìš© ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        print(f"  - ëª¨ë¦¬ê±¸ ë°ì´í„°: {morigirl_processed}ê°œ")
        print(f"  - ë¹„ëª¨ë¦¬ê±¸ ë°ì´í„°: {non_morigirl_processed}ê°œ")
        print(f"  - ì´ ë°ì´í„°: {morigirl_processed + non_morigirl_processed}ê°œ")
        
        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        result_data = {
            "folder_name": self.output_dir,
            "morigirl_count": morigirl_processed,
            "non_morigirl_count": non_morigirl_processed,
            "total_count": morigirl_processed + non_morigirl_processed,
            "max_products_per_type": max_products_per_type,
            "completion_time": str(np.datetime64('now')),
            "files_created": [
                f"{self.output_dir}/morigirl_{morigirl_processed}.npy",
                f"{self.output_dir}/non_morigirl_{non_morigirl_processed}.npy"
            ]
        }
        
        # try:
        #     result_file = f"{self.output_dir}/training_data_result.json"
        #     with open(result_file, 'w', encoding='utf-8') as f:
        #         json.dump(result_data, f, ensure_ascii=False, indent=2)
        #     print(f"ğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥: {result_file}")
        # except Exception as e:
        #     print(f"âš ï¸ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # í´ë” ê²½ë¡œ ë°˜í™˜
        return self.output_dir

    def _process_product_batch(self, products: List[Dict], data_type: str) -> int:
        """ìƒí’ˆ ë°°ì¹˜ ì²˜ë¦¬"""
        if not products:
            return 0
        
        # ë²¡í„° ì¡°íšŒ
        product_ids = [p['product_id'] for p in products]
        product_vectors = self.get_product_vectors(product_ids)
        
        # ë²¡í„°ê°€ ìˆëŠ” ìƒí’ˆë§Œ í•„í„°ë§
        valid_products = []
        for product in tqdm(products, desc=f"{data_type} ë°ì´í„° ì²˜ë¦¬"):
            product_id = product['product_id']
            
            if product_id in product_vectors:
                # íŒë§¤ ì ìˆ˜ ê³„ì‚°
                sales_score = self.calculate_sales_score(
                    product['status'], 
                    product['views'], 
                    product['impressions']
                )
                
                # ìµœì¢… ë°ì´í„° êµ¬ì„±
                product_data = {
                    'product_id': product_id,
                    'amount': product['amount'],
                    'vector': product_vectors[product_id],
                    'primary_category_id': product['primary_category_id'],
                    'secondary_category_id': product['secondary_category_id'],
                    'is_morigirl': product['is_morigirl'],
                    'sales_score': sales_score
                }
                
                valid_products.append(product_data)
        
        print(f"ğŸ’¡ {data_type}: ë²¡í„° ìˆëŠ” ìƒí’ˆ {len(valid_products)}ê°œ / ì „ì²´ {len(products)}ê°œ")
        
        # íŒŒì¼ ì €ì¥
        if valid_products:
            self.save_training_data(valid_products, data_type, len(valid_products))
        
        return len(valid_products)

    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'db_manager'):
            self.db_manager.dispose_all()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ëª¨ë¦¬ê±¸ í•™ìŠµìš© ë°ì´í„° ìƒì„±')
    parser.add_argument('--max-products', type=int, default=5000, 
                       help='ê° íƒ€ì…ë³„ ìµœëŒ€ ìƒí’ˆ ìˆ˜ (ê¸°ë³¸ê°’: 5,000ê°œ)')
    
    args = parser.parse_args()
    
    try:
        extractor = ImageVectorExtractor()
        
        print(f"ğŸš€ ëª¨ë¦¬ê±¸ í•™ìŠµìš© ë°ì´í„° ìƒì„± ì‹œì‘")
        print(f"  - ê° íƒ€ì…ë³„ ìµœëŒ€: {args.max_products:,}ê°œ")
        
        # í•™ìŠµìš© ë°ì´í„° ìƒì„±
        data_folder = extractor.process_training_data(
            max_products_per_type=args.max_products
        )
        
        print(f"\nğŸ‰ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“ ë°ì´í„° í´ë”: {data_folder}")
        print(f"ğŸ’¡ í•™ìŠµ ì‹œ --data-path {data_folder} ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        try:
            extractor.close()
        except:
            pass

if __name__ == "__main__":
    main() 