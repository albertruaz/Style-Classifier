# main.py - WandB ì¶”ê°€

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
import numpy as np
import os
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import matplotlib.pyplot as plt
from datetime import datetime
import json
import time
import wandb
from dotenv import load_dotenv

# ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ” ëª¨ë“ˆë“¤ import
from model.morigirl_model import MoriGirlClassifier, LightMorigirlCNN, get_model_info
from dataset.morigirl_dataset import MoriGirlDataset
from utils.train_utils import train_one_epoch, evaluate, save_checkpoint, load_checkpoint, calculate_class_weights, EarlyStopping

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

def initialize_wandb(config: dict) -> bool:
    """WandB ì´ˆê¸°í™”"""
    try:
        # WandB í”„ë¡œì íŠ¸ ì„¤ì •
        project_name = os.getenv('WANDB_PROJECT', 'mori-look-classification')
        entity_name = os.getenv('WANDB_ENTITY', 'albertruaz')
        
        wandb.init(
            project=project_name,
            entity=entity_name,
            config=config,
            name=f"morigirl_classifier_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            tags=['morigirl', 'classification', 'efficientnet', 'fashion'],
            notes="ëª¨ë¦¬ê±¸ ìŠ¤íƒ€ì¼ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸"
        )
        
        print("âœ… WandB ì´ˆê¸°í™” ì„±ê³µ")
        print(f"ğŸ“Š í”„ë¡œì íŠ¸: {project_name}")
        print(f"ğŸ”— ì‹¤í—˜ URL: {wandb.run.url}")
        
        return True
        
    except ImportError:
        print("âš ï¸ WandB ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"âš ï¸ WandB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

def create_data_transforms():
    """
    ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìƒì„±
    """
    # í›ˆë ¨ìš© transform (ë°ì´í„° ì¦ê°• í¬í•¨)
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # ê²€ì¦/í…ŒìŠ¤íŠ¸ìš© transform (ì¦ê°• ì—†ìŒ)
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    return train_transform, val_transform

def plot_training_history(train_losses, val_losses, train_accs, val_accs):
    """
    í•™ìŠµ ê³¼ì • ì‹œê°í™”
    """
    epochs = range(1, len(train_losses) + 1)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # Loss ê·¸ë˜í”„
    ax1.plot(epochs, train_losses, 'b-', label='Train Loss')
    ax1.plot(epochs, val_losses, 'r-', label='Val Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # Accuracy ê·¸ë˜í”„
    ax2.plot(epochs, train_accs, 'b-', label='Train Acc')
    ax2.plot(epochs, val_accs, 'r-', label='Val Acc')
    ax2.set_title('Training and Validation Accuracy')
    ax2.set_xlabel('Epochs')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    config = {
        'batch_size': 32,
        'epochs': 30,
        'learning_rate': 1e-4,
        'weight_decay': 1e-4,
        'patience': 7,  # early stopping
        'data_root': './data',  # ë°ì´í„° í´ë” ê²½ë¡œ
        'checkpoint_dir': './checkpoints',
        'num_workers': 4,
        'model_type': 'efficientnet',  # 'efficientnet' ë˜ëŠ” 'light_cnn'
        'image_size': 224,
    }
    
    # WandB ì´ˆê¸°í™”
    use_wandb = initialize_wandb(config)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    
    # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(config['checkpoint_dir'], exist_ok=True)
    
    # ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    train_transform, val_transform = create_data_transforms()
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = MoriGirlDataset(
        root_dir=config['data_root'],
        split='train',
        transform=train_transform
    )
    
    val_dataset = MoriGirlDataset(
        root_dir=config['data_root'],
        split='val',
        transform=val_transform
    )
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True,
        num_workers=config['num_workers'],
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False,
        num_workers=config['num_workers'],
        pin_memory=True
    )
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘)
    class_weights = train_dataset.get_class_weights()
    if class_weights is not None:
        class_weights = torch.tensor(class_weights).to(device)
        print(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weights}")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    if config['model_type'] == 'efficientnet':
        model = MoriGirlClassifier(pretrained=True).to(device)
    else:
        model = LightMorigirlCNN().to(device)
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    model_info = get_model_info(model)
    print(f"ğŸ“Š ëª¨ë¸ ì •ë³´:")
    print(f"  - íƒ€ì…: {config['model_type']}")
    print(f"  - ì´ íŒŒë¼ë¯¸í„°: {model_info['total_params']:,}")
    print(f"  - ëª¨ë¸ í¬ê¸°: {model_info['model_size_mb']:.2f} MB")
    
    # WandBì— ëª¨ë¸ ê°ì‹œ ì„¤ì •
    if use_wandb:
        wandb.watch(model, log='all', log_freq=10)
        wandb.log({"model_info": model_info})
    
    # ì†ì‹¤í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)
    optimizer = optim.Adam(model.parameters(), 
                          lr=config['learning_rate'], 
                          weight_decay=config['weight_decay'])
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3, verbose=True
    )
    
    # í•™ìŠµ ê¸°ë¡ìš© ë¦¬ìŠ¤íŠ¸
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    
    best_val_loss = float('inf')
    patience_counter = 0
    
    # í•™ìŠµ ë£¨í”„
    for epoch in range(config['epochs']):
        print(f"\n[Epoch {epoch+1}/{config['epochs']}]")
        
        # í›ˆë ¨
        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device
        )
        
        # ê²€ì¦
        val_loss, val_acc, val_auc = evaluate(
            model, val_loader, criterion, device
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step(val_loss)
        
        # ê¸°ë¡ ì €ì¥
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            save_checkpoint(
                model, optimizer, epoch, val_loss, 
                os.path.join(config['checkpoint_dir'], 'best_model.pth')
            )
            print("âœ… ìµœê³  ëª¨ë¸ ì €ì¥ë¨")
        else:
            patience_counter += 1
        
        # Early stopping
        if patience_counter >= config['patience']:
            print(f"Early stopping! (patience: {config['patience']})")
            break
    
    # í•™ìŠµ ê³¼ì • ì‹œê°í™”
    plot_training_history(train_losses, val_losses, train_accs, val_accs)
    
    # ìµœê³  ëª¨ë¸ ë¡œë“œ í›„ ìµœì¢… í‰ê°€
    print("\n=== ìµœì¢… í‰ê°€ ===")
    best_model = MoriGirlClassifier()
    load_checkpoint(best_model, os.path.join(config['checkpoint_dir'], 'best_model.pth'))
    best_model.to(device)
    
    final_val_loss, final_val_acc, final_val_auc = evaluate(
        best_model, val_loader, criterion, device, detailed=True
    )
    
    print(f"ìµœì¢… ê²€ì¦ Loss: {final_val_loss:.4f}")
    print(f"ìµœì¢… ê²€ì¦ Accuracy: {final_val_acc:.4f}")
    print(f"ìµœì¢… ê²€ì¦ AUC: {final_val_auc:.4f}")
    
    # ëª¨ë¸ì„ TorchScriptë¡œ ë‚´ë³´ë‚´ê¸° (ë°°í¬ìš©)
    model.eval()
    example_input = torch.randn(1, 3, 224, 224).to(device)
    traced_model = torch.jit.trace(model, example_input)
    traced_model.save("morigirl_model_traced.pt")
    print("âœ… TorchScript ëª¨ë¸ ì €ì¥ ì™„ë£Œ: morigirl_model_traced.pt")

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_path = os.path.join(config['checkpoint_dir'], f'final_model_{datetime.now().strftime("%Y%m%d_%H%M%S")}.pth')
    save_checkpoint(model, optimizer, epoch + 1, val_loss, final_path)
    
    # WandB ì•„í‹°íŒ©íŠ¸ ì €ì¥
    if use_wandb:
        try:
            # ëª¨ë¸ ì•„í‹°íŒ©íŠ¸
            model_artifact = wandb.Artifact(
                name="morigirl_classifier_model",
                type="model",
                description="ëª¨ë¦¬ê±¸ ìŠ¤íƒ€ì¼ ë¶„ë¥˜ ëª¨ë¸"
            )
            
            best_model_path = os.path.join(config['checkpoint_dir'], 'best_model.pth')
            if os.path.exists(best_model_path):
                model_artifact.add_file(best_model_path)
            
            wandb.log_artifact(model_artifact)
            print("ğŸ’¾ WandB ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ WandB ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        wandb.finish()
        print("ğŸ¯ WandB ì‹¤í—˜ ì¢…ë£Œ")

if __name__ == "__main__":
    main() 